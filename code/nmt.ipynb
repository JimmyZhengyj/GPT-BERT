{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjPTaRB4mpCd"
   },
   "source": [
    "# Colab FAQ\n",
    "\n",
    "For some basic overview and features offered in Colab notebooks, check out: [Overview of Colaboratory Features](https://colab.research.google.com/notebooks/basic_features_overview.ipynb)\n",
    "\n",
    "You need to use the colab GPU for this assignmentby selecting:\n",
    "\n",
    "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9IS9B9-yUU5"
   },
   "source": [
    "## Setup PyTorch\n",
    "All files are stored at /content/csc421/a3/ folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axbuunY8UdTB"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 625
    },
    "id": "Z-6MQhMOlHXD",
    "outputId": "29505864-012d-466d-810c-d32d2e524c55"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./opt/anaconda3/lib/python3.7/site-packages (1.5.1)\n",
      "Requirement already satisfied: torchvision in ./opt/anaconda3/lib/python3.7/site-packages (0.6.0a0+35d732a)\n",
      "Requirement already satisfied: future in ./opt/anaconda3/lib/python3.7/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in ./opt/anaconda3/lib/python3.7/site-packages (from torch) (1.18.1)\n",
      "Collecting pillow>=4.1.1\n",
      "  Using cached Pillow-8.1.2-cp37-cp37m-macosx_10_10_x86_64.whl (2.2 MB)\n",
      "Installing collected packages: pillow\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 4.0.0\n",
      "    Uninstalling Pillow-4.0.0:\n",
      "      Successfully uninstalled Pillow-4.0.0\n",
      "Successfully installed pillow-8.1.2\n",
      "Processing ./Library/Caches/pip/wheels/ec/1e/24/dbc5e4964ea99cad93230a9013d934fb5adc322c3102f69e45/Pillow-4.0.0-cp37-cp37m-macosx_10_9_x86_64.whl\n",
      "Requirement already satisfied: olefile in ./opt/anaconda3/lib/python3.7/site-packages (from Pillow==4.0.0) (0.46)\n",
      "\u001b[31mERROR: torchvision 0.6.0a0+35d732a has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: scikit-image 0.16.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: Pillow\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 8.1.2\n",
      "    Uninstalling Pillow-8.1.2:\n",
      "      Successfully uninstalled Pillow-8.1.2\n",
      "Successfully installed Pillow-4.0.0\n",
      "/Users/jimmyzheng/content/csc421/a3\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Setup python environment and change the current working directory\n",
    "######################################################################\n",
    "!pip install torch torchvision\n",
    "!pip install Pillow==4.0.0\n",
    "%mkdir -p ./content/csc421/a3/\n",
    "%cd ./content/csc421/a3\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DaTdRNuUra7"
   },
   "source": [
    "# Helper code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BIpGwANoQOg"
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "D-UJHBYZkh7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import argparse\n",
    "import pickle as pkl\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tarfile\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_file(fname,\n",
    "             origin,\n",
    "             untar=False,\n",
    "             extract=False,\n",
    "             archive_format='auto',\n",
    "             cache_dir='data'):\n",
    "    datadir = os.path.join(cache_dir)\n",
    "    if not os.path.exists(datadir):\n",
    "        os.makedirs(datadir)\n",
    "\n",
    "    if untar:\n",
    "        untar_fpath = os.path.join(datadir, fname)\n",
    "        fpath = untar_fpath + '.tar.gz'\n",
    "    else:\n",
    "        fpath = os.path.join(datadir, fname)\n",
    "    \n",
    "    print(fpath)\n",
    "    if not os.path.exists(fpath):\n",
    "        print('Downloading data from', origin)\n",
    "\n",
    "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
    "        try:\n",
    "            try:\n",
    "                urlretrieve(origin, fpath)\n",
    "            except URLError as e:\n",
    "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
    "            except HTTPError as e:\n",
    "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
    "        except (Exception, KeyboardInterrupt) as e:\n",
    "            if os.path.exists(fpath):\n",
    "                os.remove(fpath)\n",
    "            raise\n",
    "\n",
    "    if untar:\n",
    "        if not os.path.exists(untar_fpath):\n",
    "            print('Extracting file.')\n",
    "            with tarfile.open(fpath) as archive:\n",
    "                archive.extractall(datadir)\n",
    "        return untar_fpath\n",
    "\n",
    "    if extract:\n",
    "        _extract_archive(fpath, datadir, archive_format)\n",
    "\n",
    "    return fpath\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "        \n",
    "def to_var(tensor, cuda):\n",
    "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
    "\n",
    "        Arguments:\n",
    "            tensor: A Tensor object.\n",
    "            cuda: A boolean flag indicating whether to use the GPU.\n",
    "\n",
    "        Returns:\n",
    "            A Variable object, on the GPU if cuda==True.\n",
    "    \"\"\"\n",
    "    if cuda:\n",
    "        return Variable(tensor.cuda())\n",
    "    else:\n",
    "        return Variable(tensor)\n",
    "\n",
    "\n",
    "def create_dir_if_not_exists(directory):\n",
    "    \"\"\"Creates a directory if it doesn't already exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def save_loss_plot(train_losses, val_losses, opts):\n",
    "    \"\"\"Saves a plot of the training and validation loss curves.\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(train_losses)), train_losses)\n",
    "    plt.plot(range(len(val_losses)), val_losses)\n",
    "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
    "    plt.xlabel('Epochs', fontsize=16)\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_loss_comparison_lstm(l1, l2, o1, o2, fn, s=500):\n",
    "    \"\"\"Plot comparison of training and val loss curves from LSTM runs.\n",
    "    \n",
    "    Arguments:\n",
    "        l1: Tuple of lists containing training / val losses for model 1.\n",
    "        l2: Tuple of lists containing training / val losses for model 2.\n",
    "        o1: Options for model 1.\n",
    "        o2: Options for model 2.\n",
    "        fn: Output file name.\n",
    "        s: Number of training iterations to average over.\n",
    "    \"\"\"\n",
    "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
    "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    ax[0].plot(range(len(mean_l1)), mean_l1, label='ds=' + o1.data_file_name)\n",
    "    ax[0].plot(range(len(mean_l2)), mean_l2, label='ds=' + o2.data_file_name)\n",
    "    ax[0].title.set_text('Train Loss | LSTM Hidden Size = {}'.format(o2.hidden_size))\n",
    "\n",
    "    # Validation losses are assumed to be by epoch\n",
    "    ax[1].plot(range(len(l1[1])), l1[1], label='ds=' + o1.data_file_name)\n",
    "    ax[1].plot(range(len(l2[1])), l2[1], label='ds=' + o2.data_file_name)\n",
    "    ax[1].title.set_text('Val Loss | LSTM Hidden Size = {}'.format(o2.hidden_size))\n",
    "\n",
    "    ax[0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
    "    ax[0].set_ylabel(\"Loss\", fontsize=10)\n",
    "    ax[1].set_xlabel(\"Epochs\", fontsize=10)\n",
    "    ax[1].set_ylabel(\"Loss\", fontsize=10)\n",
    "    ax[0].legend(loc=\"upper right\")\n",
    "    ax[1].legend(loc=\"upper right\")\n",
    "\n",
    "    fig.suptitle('LSTM Performance by Dataset', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.85)\n",
    "    plt.legend()\n",
    "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def save_loss_comparison_by_dataset(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
    "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
    "    runs in Part 3, comparing by dataset while holding hidden size constant.\n",
    "\n",
    "    Models within each pair (l1, l2) and (l3, l4) have the same hidden sizes.\n",
    "\n",
    "    Arguments:\n",
    "        l1: Tuple of lists containing training / val losses for model 1.\n",
    "        l2: Tuple of lists containing training / val losses for model 2.\n",
    "        l3: Tuple of lists containing training / val losses for model 3.\n",
    "        l4: Tuple of lists containing training / val losses for model 4.\n",
    "        o1: Options for model 1.\n",
    "        o2: Options for model 2.\n",
    "        o3: Options for model 3.\n",
    "        o4: Options for model 4.\n",
    "        fn: Output file name.\n",
    "        s: Number of training iterations to average over.\n",
    "    \"\"\"\n",
    "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
    "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
    "    mean_l3 = [np.mean(l3[0][i*s:(i+1)*s]) for i in range(len(l3[0]) // s)]\n",
    "    mean_l4 = [np.mean(l4[0][i*s:(i+1)*s]) for i in range(len(l4[0]) // s)]\n",
    "\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label='ds=' + o1.data_file_name)\n",
    "    ax[0][0].plot(range(len(mean_l2)), mean_l2, label='ds=' + o2.data_file_name)\n",
    "    ax[0][0].title.set_text('Train Loss | Model Hidden Size = {}'.format(o1.hidden_size))\n",
    "\n",
    "    # Validation losses are assumed to be by epoch\n",
    "    ax[0][1].plot(range(len(l1[1])), l1[1], label='ds=' + o1.data_file_name)\n",
    "    ax[0][1].plot(range(len(l2[1])), l2[1], label='ds=' + o2.data_file_name)\n",
    "    ax[0][1].title.set_text('Val Loss | Model Hidden Size = {}'.format(o1.hidden_size))\n",
    "\n",
    "    ax[1][0].plot(range(len(mean_l3)), mean_l3, label='ds=' + o3.data_file_name)\n",
    "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label='ds=' + o4.data_file_name)\n",
    "    ax[1][0].title.set_text('Train Loss | Model Hidden Size = {}'.format(o3.hidden_size))\n",
    "\n",
    "    ax[1][1].plot(range(len(l3[1])), l3[1], label='ds=' + o3.data_file_name)\n",
    "    ax[1][1].plot(range(len(l4[1])), l4[1], label='ds=' + o4.data_file_name)\n",
    "    ax[1][1].title.set_text('Val Loss | Model Hidden Size = {}'.format(o4.hidden_size))\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
    "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
    "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
    "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
    "        ax[i][0].legend(loc=\"upper right\")\n",
    "        ax[i][1].legend(loc=\"upper right\")\n",
    "\n",
    "    fig.suptitle(\"Performance by Dataset Size\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    plt.legend()\n",
    "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
    "    plt.close()\n",
    "\n",
    "def save_loss_comparison_by_hidden(l1, l2, l3, l4, o1, o2, o3, o4, fn, s=500):\n",
    "    \"\"\"Plot comparison of training and validation loss curves from all four\n",
    "    runs in Part 3, comparing by hidden size while holding dataset constant.\n",
    "\n",
    "    Models within each pair (l1, l3) and (l2, l4) have the same dataset.\n",
    "\n",
    "    Arguments:\n",
    "        l1: Tuple of lists containing training / val losses for model 1.\n",
    "        l2: Tuple of lists containing training / val losses for model 2.\n",
    "        l3: Tuple of lists containing training / val losses for model 3.\n",
    "        l4: Tuple of lists containing training / val losses for model 4.\n",
    "        o1: Options for model 1.\n",
    "        o2: Options for model 2.\n",
    "        o3: Options for model 3.\n",
    "        o4: Options for model 4.\n",
    "        fn: Output file name.\n",
    "        s: Number of training iterations to average over.\n",
    "    \"\"\"\n",
    "    mean_l1 = [np.mean(l1[0][i*s:(i+1)*s]) for i in range(len(l1[0]) // s)]\n",
    "    mean_l2 = [np.mean(l2[0][i*s:(i+1)*s]) for i in range(len(l2[0]) // s)]\n",
    "    mean_l3 = [np.mean(l3[0][i*s:(i+1)*s]) for i in range(len(l3[0]) // s)]\n",
    "    mean_l4 = [np.mean(l4[0][i*s:(i+1)*s]) for i in range(len(l4[0]) // s)]\n",
    "\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "\n",
    "    ax[0][0].plot(range(len(mean_l1)), mean_l1, label='hid_size=' + str(o1.hidden_size))\n",
    "    ax[0][0].plot(range(len(mean_l3)), mean_l3, label='hid_size=' + str(o3.hidden_size))\n",
    "    ax[0][0].title.set_text('Train Loss | Dataset = ' + o1.data_file_name)\n",
    "\n",
    "    # Validation losses are assumed to be by epoch\n",
    "    ax[0][1].plot(range(len(l1[1])), l1[1], label='hid_size=' + str(o1.hidden_size))\n",
    "    ax[0][1].plot(range(len(l3[1])), l3[1], label='hid_size=' + str(o3.hidden_size))\n",
    "    ax[0][1].title.set_text('Val Loss | Dataset = ' + o1.data_file_name)\n",
    "\n",
    "    ax[1][0].plot(range(len(mean_l2)), mean_l2, label='hid_size=' + str(o2.hidden_size))\n",
    "    ax[1][0].plot(range(len(mean_l4)), mean_l4, label='hid_size=' + str(o4.hidden_size))\n",
    "    ax[1][0].title.set_text('Train Loss | Dataset = ' + o3.data_file_name)\n",
    "\n",
    "    ax[1][1].plot(range(len(l2[1])), l2[1], label='hid_size=' + str(o2.hidden_size))\n",
    "    ax[1][1].plot(range(len(l4[1])), l4[1], label='hid_size=' + str(o4.hidden_size))\n",
    "    ax[1][1].title.set_text('Val Loss | Dataset = ' + o4.data_file_name)\n",
    "\n",
    "    for i in range(2):\n",
    "        ax[i][0].set_xlabel(\"Iterations (x{})\".format(s), fontsize=10)\n",
    "        ax[i][0].set_ylabel(\"Loss\", fontsize=10)\n",
    "        ax[i][1].set_xlabel(\"Epochs\", fontsize=10)\n",
    "        ax[i][1].set_ylabel(\"Loss\", fontsize=10)\n",
    "        ax[i][0].legend(loc=\"upper right\")\n",
    "        ax[i][1].legend(loc=\"upper right\")\n",
    "\n",
    "    fig.suptitle(\"Performance by Hidden State Size\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    plt.legend()\n",
    "    plt.savefig('./loss_plot_{}.pdf'.format(fn))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def checkpoint(encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
    "    contains the char_to_index and index_to_char mappings, and the start_token\n",
    "    and end_token values.\n",
    "    \"\"\"\n",
    "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
    "        torch.save(encoder, f)\n",
    "\n",
    "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
    "        torch.save(decoder, f)\n",
    "\n",
    "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
    "        pkl.dump(idx_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbvpn4MaV0I1"
   },
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XVT4TNTOV3Eg"
   },
   "outputs": [],
   "source": [
    "def read_lines(filename):\n",
    "    \"\"\"Read a file and split it into lines.\n",
    "    \"\"\"\n",
    "    lines = open(filename).read().strip().lower().split('\\n')\n",
    "    return lines\n",
    "\n",
    "\n",
    "def read_pairs(filename):\n",
    "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
    "\n",
    "    Returns:\n",
    "        source_words: A list of the first word in each line of the file.\n",
    "        target_words: A list of the second word in each line of the file.\n",
    "    \"\"\"\n",
    "    lines = read_lines(filename)\n",
    "    source_words, target_words = [], []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            source, target = line.split()\n",
    "            source_words.append(source)\n",
    "            target_words.append(target)\n",
    "    return source_words, target_words\n",
    "\n",
    "\n",
    "def all_alpha_or_dash(s):\n",
    "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
    "    \"\"\"\n",
    "    return all(c.isalpha() or c == '-' for c in s)\n",
    "\n",
    "\n",
    "def filter_lines(lines):\n",
    "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
    "    \"\"\"\n",
    "    return [line for line in lines if all_alpha_or_dash(line)]\n",
    "\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\n",
    "    \"\"\"\n",
    "    path = \"./data/{}.txt\".format(file_name)\n",
    "    source_lines, target_lines = read_pairs(path)\n",
    "\n",
    "    # Filter lines\n",
    "    source_lines = filter_lines(source_lines)\n",
    "    target_lines = filter_lines(target_lines)\n",
    "\n",
    "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
    "\n",
    "    # Create a dictionary mapping each character to a unique index\n",
    "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
    "\n",
    "    # Add start and end tokens to the dictionary\n",
    "    start_token = len(char_to_index)\n",
    "    end_token = len(char_to_index) + 1\n",
    "    char_to_index['SOS'] = start_token\n",
    "    char_to_index['EOS'] = end_token\n",
    "\n",
    "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
    "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
    "\n",
    "    # Store the final size of the vocabulary\n",
    "    vocab_size = len(char_to_index)\n",
    "\n",
    "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
    "\n",
    "    idx_dict = { 'char_to_index': char_to_index,\n",
    "                 'index_to_char': index_to_char,\n",
    "                 'start_token': start_token,\n",
    "                 'end_token': end_token }\n",
    "\n",
    "    return line_pairs, vocab_size, idx_dict\n",
    "\n",
    "\n",
    "def create_dict(pairs):\n",
    "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
    "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
    "    all source indexes and the other containing all corresponding target indexes.\n",
    "    Within a batch, all the source words are the same length, and all the target words are\n",
    "    the same length.\n",
    "    \"\"\"\n",
    "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
    "\n",
    "    d = defaultdict(list)\n",
    "    for (s,t) in unique_pairs:\n",
    "        d[(len(s), len(t))].append((s,t))\n",
    "\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRWfRdmVVjUl"
   },
   "source": [
    "## Training and evaluation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wa5-onJhoSeM"
   },
   "outputs": [],
   "source": [
    "def string_to_index_list(s, char_to_index, end_token):\n",
    "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
    "    \"\"\"\n",
    "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
    "\n",
    "\n",
    "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
    "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
    "    word independently, and then stitching the words back together with spaces between them.\n",
    "    \"\"\"\n",
    "    if idx_dict is None:\n",
    "      line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
    "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
    "\n",
    "\n",
    "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Translates a given string from English to Pig-Latin.\n",
    "    \"\"\"\n",
    "\n",
    "    char_to_index = idx_dict['char_to_index']\n",
    "    index_to_char = idx_dict['index_to_char']\n",
    "    start_token = idx_dict['start_token']\n",
    "    end_token = idx_dict['end_token']\n",
    "\n",
    "    max_generated_chars = 20\n",
    "    gen_string = ''\n",
    "\n",
    "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
    "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
    "\n",
    "    encoder_annotations, encoder_last_hidden, encoder_last_cell = encoder(indexes)\n",
    "\n",
    "    decoder_hidden = encoder_last_hidden\n",
    "    decoder_cell = encoder_last_cell\n",
    "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
    "    decoder_inputs = decoder_input\n",
    "\n",
    "    for i in range(max_generated_chars):\n",
    "        ## slow decoding, recompute everything at each time\n",
    "        decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
    "\n",
    "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
    "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
    "        ni = ni[-1] #latest output token\n",
    "\n",
    "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
    "\n",
    "        if ni == end_token:\n",
    "            break\n",
    "        else:\n",
    "            gen_string = \"\".join(\n",
    "                [index_to_char[int(item)] \n",
    "                for item in generated_words.cpu().numpy().reshape(-1)])\n",
    "\n",
    "    return gen_string\n",
    "\n",
    "\n",
    "def visualize_attention(input_string, encoder, decoder, idx_dict, opts):\n",
    "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\n",
    "    \"\"\"\n",
    "    if idx_dict is None:\n",
    "      line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
    "    char_to_index = idx_dict['char_to_index']\n",
    "    index_to_char = idx_dict['index_to_char']\n",
    "    start_token = idx_dict['start_token']\n",
    "    end_token = idx_dict['end_token']\n",
    "\n",
    "    max_generated_chars = 20\n",
    "    gen_string = ''\n",
    "\n",
    "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
    "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
    "\n",
    "    encoder_annotations, encoder_hidden, encoder_cell = encoder(indexes)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_cell = encoder_cell\n",
    "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
    "    decoder_inputs = decoder_input\n",
    "\n",
    "    produced_end_token = False\n",
    "\n",
    "    for i in range(max_generated_chars):\n",
    "        ## slow decoding, recompute everything at each time\n",
    "        decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
    "        generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
    "        ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
    "        ni = ni[-1] #latest output token\n",
    "\n",
    "        decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
    "\n",
    "        if ni == end_token:\n",
    "            break\n",
    "        else:\n",
    "            gen_string = \"\".join(\n",
    "                [index_to_char[int(item)] \n",
    "                for item in generated_words.cpu().numpy().reshape(-1)])\n",
    "    \n",
    "    if isinstance(attention_weights, tuple):\n",
    "      ## transformer's attention mweights\n",
    "      attention_weights, self_attention_weights = attention_weights\n",
    "    \n",
    "    all_attention_weights = attention_weights.data.cpu().numpy()\n",
    "    \n",
    "    for i in range(len(all_attention_weights)):\n",
    "        attention_weights_matrix = all_attention_weights[i].squeeze()\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(attention_weights_matrix, cmap='bone')\n",
    "        fig.colorbar(cax)\n",
    "\n",
    "        # Set up axes\n",
    "        ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\n",
    "        ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\n",
    "\n",
    "        # Show label at every tick\n",
    "        ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "        # Add title\n",
    "        plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\n",
    "        plt.tight_layout()\n",
    "        plt.grid('off')\n",
    "        plt.show()\n",
    "\n",
    "    return gen_string\n",
    "\n",
    "\n",
    "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
    "    \"\"\"Train/Evaluate the model on a dataset.\n",
    "\n",
    "    Arguments:\n",
    "        data_dict: The validation/test word pairs, organized by source and target lengths.\n",
    "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
    "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
    "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
    "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
    "        optimizer: Train the weights if an optimizer is given. None if only evaluate the model. \n",
    "        opts: The command-line arguments.\n",
    "\n",
    "    Returns:\n",
    "        mean_loss: The average loss over all batches from data_dict.\n",
    "    \"\"\"\n",
    "    start_token = idx_dict['start_token']\n",
    "    end_token = idx_dict['end_token']\n",
    "    char_to_index = idx_dict['char_to_index']\n",
    "\n",
    "    losses = []\n",
    "    for key in data_dict:\n",
    "        input_strings, target_strings = zip(*data_dict[key])\n",
    "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
    "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
    "\n",
    "        num_tensors = len(input_tensors)\n",
    "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
    "\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            start = i * opts.batch_size\n",
    "            end = start + opts.batch_size\n",
    "\n",
    "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
    "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
    "\n",
    "            # The batch size may be different in each epoch\n",
    "            BS = inputs.size(0)\n",
    "\n",
    "            encoder_annotations, encoder_hidden, encoder_cell = encoder(inputs)\n",
    "\n",
    "            # The last hidden state of the encoder becomes the first hidden state of the decoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "            decoder_cell = encoder_cell\n",
    "\n",
    "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  # BS x 1 --> 16x1  CHECKED\n",
    "            decoder_input = to_var(start_vector, opts.cuda)  # BS x 1 --> 16x1  CHECKED\n",
    "\n",
    "            loss = 0.0\n",
    "\n",
    "            seq_len = targets.size(1)  # Gets seq_len from BS x seq_len\n",
    "\n",
    "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  # Gets decoder inputs by shifting the targets to the right \n",
    "\n",
    "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden, decoder_cell)\n",
    "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
    "            targets_flatten = targets.view(-1)\n",
    "            \n",
    "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            ## training if an optimizer is provided\n",
    "            if optimizer:\n",
    "              # Zero gradients\n",
    "              optimizer.zero_grad()\n",
    "              # Compute gradients\n",
    "              loss.backward()\n",
    "              # Update the parameters of the encoder and decoder\n",
    "              optimizer.step()\n",
    "\n",
    "    return losses\n",
    "\n",
    "  \n",
    "\n",
    "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
    "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
    "        * Prints training and val loss each epoch.\n",
    "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
    "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
    "        * Returns loss curves for comparison\n",
    "\n",
    "    Arguments:\n",
    "        train_dict: The training word pairs, organized by source and target lengths.\n",
    "        val_dict: The validation word pairs, organized by source and target lengths.\n",
    "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
    "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
    "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
    "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
    "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
    "        opts: The command-line arguments.\n",
    "    \n",
    "    Returns:\n",
    "        losses: Lists containing training and validation loss curves.\n",
    "    \"\"\"\n",
    "\n",
    "    start_token = idx_dict['start_token']\n",
    "    end_token = idx_dict['end_token']\n",
    "    char_to_index = idx_dict['char_to_index']\n",
    "\n",
    "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
    "\n",
    "    best_val_loss = 1e6\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    mean_train_losses = []\n",
    "    mean_val_losses = []\n",
    "\n",
    "    early_stopping_counter = 0\n",
    "    \n",
    "    for epoch in range(opts.nepochs):\n",
    "\n",
    "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
    "        \n",
    "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
    "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
    "\n",
    "        mean_train_loss = np.mean(train_loss)\n",
    "        mean_val_loss = np.mean(val_loss)\n",
    "\n",
    "        if mean_val_loss < best_val_loss:\n",
    "            checkpoint(encoder, decoder, idx_dict, opts)\n",
    "            best_val_loss = mean_val_loss\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "        \n",
    "        if early_stopping_counter > opts.early_stopping_patience:\n",
    "            print(\"Validation loss has not improved in {} epochs, stopping early\".format(opts.early_stopping_patience))\n",
    "            print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
    "            return (train_losses, mean_val_losses)\n",
    "\n",
    "        gen_string = translate_sentence(TEST_SENTENCE, encoder, decoder, idx_dict, opts)\n",
    "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, mean_train_loss, mean_val_loss, gen_string))\n",
    "\n",
    "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
    "        loss_log.flush()\n",
    "\n",
    "        train_losses += train_loss\n",
    "        val_losses += val_loss\n",
    "\n",
    "        mean_train_losses.append(mean_train_loss)\n",
    "        mean_val_losses.append(mean_val_loss)\n",
    "\n",
    "        save_loss_plot(mean_train_losses, mean_val_losses, opts)\n",
    "\n",
    "    print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
    "    return (train_losses, mean_val_losses)\n",
    "\n",
    "\n",
    "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
    "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\n",
    "    \"\"\"\n",
    "    print('=' * 80)\n",
    "    print('Data Stats'.center(80))\n",
    "    print('-' * 80)\n",
    "    for pair in line_pairs[:5]:\n",
    "        print(pair)\n",
    "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
    "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
    "    print('Vocab size: {}'.format(vocab_size))\n",
    "    print('=' * 80)\n",
    "\n",
    "\n",
    "def train(opts):\n",
    "    line_pairs, vocab_size, idx_dict = load_data(opts['data_file_name'])\n",
    "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
    "\n",
    "    # Split the line pairs into an 80% train and 20% val split\n",
    "    num_lines = len(line_pairs)\n",
    "    num_train = int(0.8 * num_lines)\n",
    "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
    "\n",
    "    # Group the data by the lengths of the source and target words, to form batches\n",
    "    train_dict = create_dict(train_pairs)\n",
    "    val_dict = create_dict(val_pairs)\n",
    "\n",
    "    ##########################################################################\n",
    "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
    "    ##########################################################################\n",
    "    if opts.encoder_type == \"rnn\":\n",
    "        encoder = LSTMEncoder(vocab_size=vocab_size, \n",
    "                              hidden_size=opts.hidden_size, \n",
    "                              opts=opts)\n",
    "    elif opts.encoder_type == \"transformer\":\n",
    "        encoder = TransformerEncoder(vocab_size=vocab_size, \n",
    "                                     hidden_size=opts.hidden_size, \n",
    "                                     num_layers=opts.num_transformer_layers,\n",
    "                                     opts=opts)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if opts.decoder_type == 'rnn':\n",
    "        decoder = RNNDecoder(vocab_size=vocab_size, \n",
    "                             hidden_size=opts.hidden_size)\n",
    "    elif opts.decoder_type == 'rnn_attention':\n",
    "        decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
    "                                      hidden_size=opts.hidden_size, \n",
    "                                      attention_type=opts.attention_type)\n",
    "    elif opts.decoder_type == 'transformer':\n",
    "        decoder = TransformerDecoder(vocab_size=vocab_size, \n",
    "                                     hidden_size=opts.hidden_size, \n",
    "                                     num_layers=opts.num_transformer_layers)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    #### setup checkpoint path\n",
    "    model_name = 'h{}-bs{}-{}-{}'.format(opts.hidden_size, \n",
    "                                      opts.batch_size, \n",
    "                                      opts.decoder_type,\n",
    "                                      opts.data_file_name)\n",
    "    opts.checkpoint_path = model_name\n",
    "    create_dir_if_not_exists(opts.checkpoint_path)\n",
    "    ####\n",
    "\n",
    "    if opts.cuda:\n",
    "        encoder.cuda()\n",
    "        decoder.cuda()\n",
    "        print(\"Moved models to GPU!\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
    "\n",
    "    try:\n",
    "        losses = training_loop(train_dict, val_dict, idx_dict, encoder, \n",
    "                               decoder, criterion, optimizer, opts)\n",
    "    except KeyboardInterrupt:\n",
    "        print('Exiting early from training.')\n",
    "        return encoder, decoder, losses\n",
    "      \n",
    "    return encoder, decoder, losses\n",
    "\n",
    "\n",
    "def print_opts(opts):\n",
    "    \"\"\"Prints the values of all command-line arguments.\n",
    "    \"\"\"\n",
    "    print('=' * 80)\n",
    "    print('Opts'.center(80))\n",
    "    print('-' * 80)\n",
    "    for key in opts.__dict__:\n",
    "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
    "    print('=' * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yh08KhgnA30"
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aROU2xZanDKq",
    "outputId": "72e5912c-aebb-4050-b2f3-4ca1d3bdf9cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/pig_latin_small.txt\n",
      "data/pig_latin_large.txt\n"
     ]
    }
   ],
   "source": [
    "######################################################################\n",
    "# Download Translation datasets\n",
    "######################################################################\n",
    "data_fpath = get_file(fname='pig_latin_small.txt', \n",
    "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_small.txt', \n",
    "                         untar=False)\n",
    "\n",
    "data_fpath = get_file(fname='pig_latin_large.txt', \n",
    "                         origin='http://www.cs.toronto.edu/~jba/pig_latin_large.txt', \n",
    "                         untar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDYMr7NclZdw"
   },
   "source": [
    "# Part 1: Long Short-Term Memory Unit (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCae1mOUlZrC"
   },
   "source": [
    "## Step 1: LSTM Cell\n",
    "Please implement the Long Short-Term Memory class defined in the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cOnALRQkkjDO"
   },
   "outputs": [],
   "source": [
    "class MyLSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MyLSTMCell, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # ------------\n",
    "        # FILL THIS IN\n",
    "        # ------------\n",
    "        self.Wii = nn.Linear(input_size, hidden_size)\n",
    "        self.Whi = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.Wif = nn.Linear(input_size, hidden_size)\n",
    "        self.Whf = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.Wig = nn.Linear(input_size, hidden_size)\n",
    "        self.Whg = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.Wio = nn.Linear(input_size, hidden_size)\n",
    "        self.Who = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        \"\"\"Forward pass of the LSTM computation for one time step.\n",
    "\n",
    "        Arguments\n",
    "            x: batch_size x input_size\n",
    "            h_prev: batch_size x hidden_size\n",
    "            c_prev: batch_size x hidden_size\n",
    "\n",
    "        Returns:\n",
    "            h_new: batch_size x hidden_size\n",
    "            c_new: batch_size x hidden_size\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # FILL THIS IN\n",
    "        # ------------\n",
    "        i = torch.sigmoid(self.Wii(x) + self.Whi(h_prev))\n",
    "        f = torch.sigmoid(self.Wif(x) + self.Whf(h_prev))\n",
    "        g = torch.tanh(self.Wig(x) + self.Who(h_prev))\n",
    "        o = torch.sigmoid(self.Wio(x) + self.Who(h_prev))\n",
    "        c_new = f * c_prev + i * g\n",
    "        h_new = o * torch.tanh(c_new)\n",
    "        return h_new, c_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecEq4TP2lZ4Z"
   },
   "source": [
    "## Step 2: LSTM Encoder\n",
    "Please inspect the following recurrent encoder/decoder implementations. Make sure to run the cells before proceeding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8jDNim2fmVJV"
   },
   "outputs": [],
   "source": [
    "class LSTMEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, opts):\n",
    "        super(LSTMEncoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.opts = opts\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.lstm = MyLSTMCell(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass of the encoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
    "\n",
    "        Returns:\n",
    "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len = inputs.size()\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        cell = self.init_hidden(batch_size)\n",
    "\n",
    "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "        annotations = []\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
    "            hidden, cell = self.lstm(x, hidden, cell)\n",
    "            annotations.append(hidden)\n",
    "\n",
    "        annotations = torch.stack(annotations, dim=1)\n",
    "        return annotations, hidden, cell\n",
    "\n",
    "    def init_hidden(self, bs):\n",
    "        \"\"\"Creates a tensor of zeros to represent the initial hidden states\n",
    "        of a batch of sequences.\n",
    "\n",
    "        Arguments:\n",
    "            bs: The batch size for the initial hidden state.\n",
    "\n",
    "        Returns:\n",
    "            hidden: An initial hidden state of all zeros. (batch_size x hidden_size)\n",
    "        \"\"\"\n",
    "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HvwizYM9ma4p"
   },
   "outputs": [],
   "source": [
    "class RNNDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size):\n",
    "        super(RNNDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = MyLSTMCell(input_size=hidden_size, hidden_size=hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
    "        \"\"\"Forward pass of the non-attentional decoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch. (batch_size x seq_len)\n",
    "            annotations: This is not used here. It just maintains consistency with the\n",
    "                    interface used by the AttentionDecoder class.\n",
    "            hidden_init: The hidden states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
    "            cell_init: The cell states from the last step of encoder, across a batch. (batch_size x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            None\n",
    "        \"\"\"        \n",
    "        batch_size, seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
    "\n",
    "        hiddens = []\n",
    "        h_prev = hidden_init\n",
    "        c_prev = cell_init\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            x = embed[:,i,:]  # Get the current time step input tokens, across the whole batch\n",
    "            h_prev, c_prev = self.rnn(x, h_prev, c_prev)  # batch_size x hidden_size\n",
    "            hiddens.append(h_prev)\n",
    "\n",
    "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
    "        \n",
    "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
    "        return output, None  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSDTbsydlaGI"
   },
   "source": [
    "## Step 3: Training and Analysis\n",
    "Train the following language model comprised of recurrent encoder and decoders. \n",
    "\n",
    "First, we train on the smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XmVuXTozTPF7",
    "outputId": "a22f290e-6f6c-4403-a0f9-2499d0e6cf29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_small                        \n",
      "                                   cuda: 0                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.005                                  \n",
      "                               lr_decay: 0.99                                   \n",
      "                early_stopping_patience: 20                                     \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 32                                     \n",
      "                           encoder_type: rnn                                    \n",
      "                           decoder_type: rnn                                    \n",
      "                         attention_type:                                        \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('fears', 'earsfay')\n",
      "('utility', 'utilityway')\n",
      "('accusation', 'accusationway')\n",
      "('i', 'iway')\n",
      "('delirium', 'eliriumday')\n",
      "Num unique word pairs: 3198\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Epoch:   0 | Train loss: 2.422 | Val loss: 2.044 | Gen: eay ay insay-inay ay ay-ay\n",
      "Epoch:   1 | Train loss: 1.863 | Val loss: 1.815 | Gen: ay-onsay ay-oay illensay-onsay ileday onsay-onsay\n",
      "Epoch:   2 | Train loss: 1.664 | Val loss: 1.662 | Gen: eray ay-ay-ay-ay-ay-ay onsingsay-ingway ilay onsay-ouray\n",
      "Epoch:   3 | Train loss: 1.518 | Val loss: 1.563 | Gen: etay ay-ay-ay-ay-ay-ay onsingsay-ingway ilway ontedway\n",
      "Epoch:   4 | Train loss: 1.387 | Val loss: 1.523 | Gen: etway alway onsingsay-ingsay ilway oningway\n",
      "Epoch:   5 | Train loss: 1.272 | Val loss: 1.431 | Gen: ethay away-ay oncingsay-ingsay isway oringway\n",
      "Epoch:   6 | Train loss: 1.177 | Val loss: 1.337 | Gen: etway away-ay oncingsay-ingsay isway oringway\n",
      "Epoch:   7 | Train loss: 1.077 | Val loss: 1.281 | Gen: ethay away-ay oncicationday isway ortingway\n",
      "Epoch:   8 | Train loss: 0.996 | Val loss: 1.219 | Gen: ehay away-alyway onsingsay-ingday isway orway-acky\n",
      "Epoch:   9 | Train loss: 0.922 | Val loss: 1.176 | Gen: ethay ay-arway oncictionday istay oringway\n",
      "Epoch:  10 | Train loss: 0.861 | Val loss: 1.148 | Gen: ehay ay-arway onsingsay-oday isway oringway\n",
      "Epoch:  11 | Train loss: 0.811 | Val loss: 1.129 | Gen: ehay ay-alway onsiondingway isway orway-acky\n",
      "Epoch:  12 | Train loss: 0.764 | Val loss: 1.108 | Gen: ehay away-ay onicitednay-atway isway orfway\n",
      "Epoch:  13 | Train loss: 0.726 | Val loss: 1.059 | Gen: ehay airway oncicitedgay isway orenway\n",
      "Epoch:  14 | Train loss: 0.656 | Val loss: 1.013 | Gen: ehay ay-arway onicingdray-acy isway orway-asway\n",
      "Epoch:  15 | Train loss: 0.610 | Val loss: 1.027 | Gen: ehay ay-arway oncictiondray isway orfreway\n",
      "Epoch:  16 | Train loss: 0.592 | Val loss: 0.977 | Gen: ehtay ay-aritway onicingtionway isway orgay-alyway\n",
      "Epoch:  17 | Train loss: 0.564 | Val loss: 1.009 | Gen: ehay airway oncictingdray isway oringway\n",
      "Epoch:  18 | Train loss: 0.544 | Val loss: 1.010 | Gen: ehay ay-arway oniciongray-ingay isway oringway\n",
      "Epoch:  19 | Train loss: 0.519 | Val loss: 0.958 | Gen: ehtay airway onicingredcay isway oringhay\n",
      "Epoch:  20 | Train loss: 0.496 | Val loss: 0.959 | Gen: ehay airway onicingray-icetay isway orfway-angway\n",
      "Epoch:  21 | Train loss: 0.473 | Val loss: 0.973 | Gen: ehtay airway oniciondentday isway oringhay\n",
      "Epoch:  22 | Train loss: 0.457 | Val loss: 1.062 | Gen: ehay airway ondicingtionway isway orringway\n",
      "Epoch:  23 | Train loss: 0.444 | Val loss: 0.956 | Gen: ehtay airway oniciondertay isway oringfay\n",
      "Epoch:  24 | Train loss: 0.426 | Val loss: 1.000 | Gen: ehatway airway onicitonday-answay isway orfingway\n",
      "Epoch:  25 | Train loss: 0.398 | Val loss: 0.970 | Gen: ehtay airway oniciongringway isway orfingway\n",
      "Epoch:  26 | Train loss: 0.376 | Val loss: 0.950 | Gen: ehtay airway oniciongringway isway orfingway\n",
      "Epoch:  27 | Train loss: 0.356 | Val loss: 0.973 | Gen: ehtay airway onicingdontway isway oringlay\n",
      "Epoch:  28 | Train loss: 0.345 | Val loss: 0.926 | Gen: ehay airway onicingdentcay isway orkingway\n",
      "Epoch:  29 | Train loss: 0.337 | Val loss: 0.981 | Gen: ehtay airway ondicitingonway isway oringfay\n",
      "Epoch:  30 | Train loss: 0.332 | Val loss: 0.960 | Gen: ehtay airway ondicintingbay isway orkingway\n",
      "Epoch:  31 | Train loss: 0.316 | Val loss: 0.920 | Gen: ehay airway ondicitingnay isway oringlay\n",
      "Epoch:  32 | Train loss: 0.306 | Val loss: 0.917 | Gen: ehtay airway ondicintedfay isway oringlay\n",
      "Epoch:  33 | Train loss: 0.307 | Val loss: 0.973 | Gen: ehay airway onincigtionmay isway oringfay\n",
      "Epoch:  34 | Train loss: 0.313 | Val loss: 0.924 | Gen: ethay ariway ondicintedfay isway oringlay\n",
      "Epoch:  35 | Train loss: 0.294 | Val loss: 0.918 | Gen: ethay airway oniciongringway isway orkingway\n",
      "Epoch:  36 | Train loss: 0.282 | Val loss: 0.953 | Gen: ethay airway ondicitingenay isway oringlay\n",
      "Epoch:  37 | Train loss: 0.284 | Val loss: 0.902 | Gen: ethay airway ondicintedfay isway oringlay\n",
      "Epoch:  38 | Train loss: 0.268 | Val loss: 0.910 | Gen: ethay airway ondicingtionway isway oringfay\n",
      "Epoch:  39 | Train loss: 0.255 | Val loss: 0.917 | Gen: ehtay airway ondicitingneday isway orikngway\n",
      "Epoch:  40 | Train loss: 0.243 | Val loss: 0.905 | Gen: ethay airway ondicitinngblay isway oringlay\n",
      "Epoch:  41 | Train loss: 0.231 | Val loss: 0.868 | Gen: ethay airway ondicitencedway isway orkingway\n",
      "Epoch:  42 | Train loss: 0.220 | Val loss: 0.936 | Gen: ethay airway onintidingcay isway oriknay\n",
      "Epoch:  43 | Train loss: 0.213 | Val loss: 0.900 | Gen: ethay airway ondicitingenay isway orkingway\n",
      "Epoch:  44 | Train loss: 0.209 | Val loss: 0.910 | Gen: ethay airway onintidingcay isway orkingway\n",
      "Epoch:  45 | Train loss: 0.205 | Val loss: 0.908 | Gen: ethay airway onintidedfay-ackay isway orkingway\n",
      "Epoch:  46 | Train loss: 0.208 | Val loss: 0.959 | Gen: ethay airway ondicitenglay isway orkingway\n",
      "Epoch:  47 | Train loss: 0.213 | Val loss: 0.961 | Gen: ethay airway onintidedctnay isway orkingway\n",
      "Epoch:  48 | Train loss: 0.225 | Val loss: 0.926 | Gen: ethay airway onidintectinay isway orkingway\n",
      "Epoch:  49 | Train loss: 0.233 | Val loss: 0.936 | Gen: ethay airway ondicitingnay isway orikngray\n",
      "Obtained lowest validation loss of: 0.8680315683255109\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay airway ondicitingnay isway orikngray\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "\n",
    "rnn_args_s = AttrDict()\n",
    "args_dict = {\n",
    "              'data_file_name': 'pig_latin_small',\n",
    "              'cuda':False,\n",
    "              'nepochs':50,\n",
    "              'checkpoint_dir':\"checkpoints\",\n",
    "              'learning_rate':0.005,\n",
    "              'lr_decay':0.99,\n",
    "              'early_stopping_patience':20,\n",
    "              'batch_size':64,\n",
    "              'hidden_size':32,\n",
    "              'encoder_type': 'rnn', # options: rnn / transformer\n",
    "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
    "              'attention_type': '',  # options: additive / scaled_dot\n",
    "}\n",
    "rnn_args_s.update(args_dict)\n",
    "\n",
    "print_opts(rnn_args_s)\n",
    "rnn_encode_s, rnn_decoder_s, rnn_losses_s = train(rnn_args_s)\n",
    "\n",
    "translated = translate_sentence(TEST_SENTENCE, rnn_encode_s, rnn_decoder_s, None, rnn_args_s)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mR97V_NtER6"
   },
   "source": [
    "Next, we train on the larger dataset. This experiment investigates if increasing dataset size improves model generalization on the validation set. \n",
    "\n",
    "For a fair comparison, the number of iterations (not number of epochs) for each run should be similar. This is done in a rough and dirty way by adjusting the batch size so approximately the same number of batches is processed per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H3YLrAjsmx_W",
    "outputId": "71599da0-683e-4161-d7d9-74f923ef05ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_large                        \n",
      "                                   cuda: 0                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.005                                  \n",
      "                               lr_decay: 0.99                                   \n",
      "                early_stopping_patience: 10                                     \n",
      "                             batch_size: 512                                    \n",
      "                            hidden_size: 32                                     \n",
      "                           encoder_type: rnn                                    \n",
      "                           decoder_type: rnn                                    \n",
      "                         attention_type:                                        \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('tutor', 'utortay')\n",
      "('es', 'esway')\n",
      "('dams', 'amsday')\n",
      "('gala', 'alagay')\n",
      "('rehearsal', 'ehearsalray')\n",
      "Num unique word pairs: 22402\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Epoch:   0 | Train loss: 2.363 | Val loss: 2.091 | Gen: ay-onay ay-onay ontay-ontay-onay onay-ay ontay-onay-ay\n",
      "Epoch:   1 | Train loss: 1.874 | Val loss: 1.962 | Gen: ay-ay ay-ay-ay ontingingererereray- insay ontingay-ontay\n",
      "Epoch:   2 | Train loss: 1.684 | Val loss: 1.816 | Gen: ay-ay ay-ay ontiontintinteray istay ontiontay-ay\n",
      "Epoch:   3 | Train loss: 1.539 | Val loss: 1.697 | Gen: ay-ay ay-ay ontingingray-ingeray istay oudingay\n",
      "Epoch:   4 | Train loss: 1.424 | Val loss: 1.604 | Gen: ay-ay aray ontingingray issay odgingray\n",
      "Epoch:   5 | Train loss: 1.300 | Val loss: 1.525 | Gen: eray away-ay ontioninstay-away isway ougingray\n",
      "Epoch:   6 | Train loss: 1.198 | Val loss: 1.437 | Gen: eray ay-awyway ontioningway-yway isway ougingray\n",
      "Epoch:   7 | Train loss: 1.116 | Val loss: 1.438 | Gen: eray arday ontinininscay isway oudgishay\n",
      "Epoch:   8 | Train loss: 1.044 | Val loss: 1.347 | Gen: eray ariway ontiningray-incay isway odgishay\n",
      "Epoch:   9 | Train loss: 0.971 | Val loss: 1.323 | Gen: eray arway ontiningingscay isway odginscay\n",
      "Epoch:  10 | Train loss: 0.917 | Val loss: 1.297 | Gen: erhay ariway ontiningingray isway ungigsway\n",
      "Epoch:  11 | Train loss: 0.861 | Val loss: 1.282 | Gen: erhay arway oniningiconway isway odginscay\n",
      "Epoch:  12 | Train loss: 0.814 | Val loss: 1.250 | Gen: ehay ariway onininicondcay isway odgistay\n",
      "Epoch:  13 | Train loss: 0.799 | Val loss: 1.225 | Gen: erhay ariway oniningicondcay isway odgincay\n",
      "Epoch:  14 | Train loss: 0.768 | Val loss: 1.186 | Gen: ethay ariway ondinicingenay isway odgingsay\n",
      "Epoch:  15 | Train loss: 0.721 | Val loss: 1.172 | Gen: ethay ariway oniningiconday isway odingpay\n",
      "Epoch:  16 | Train loss: 0.683 | Val loss: 1.110 | Gen: ehay ariway ondinicionday isway odingshay\n",
      "Epoch:  17 | Train loss: 0.651 | Val loss: 1.101 | Gen: heay ariway odiningincay isway odigray\n",
      "Epoch:  18 | Train loss: 0.621 | Val loss: 1.102 | Gen: ehay ariway ondinicineday isway odingray\n",
      "Epoch:  19 | Train loss: 0.617 | Val loss: 1.133 | Gen: ethay airway odininingceray isway odingray\n",
      "Epoch:  20 | Train loss: 0.607 | Val loss: 1.130 | Gen: ethay airway odintionictway isway odigray\n",
      "Epoch:  21 | Train loss: 0.587 | Val loss: 1.149 | Gen: ethay airway ondinictionway isway odingray\n",
      "Epoch:  22 | Train loss: 0.584 | Val loss: 1.092 | Gen: ethay ariway odininingtay isway ordigshay\n",
      "Epoch:  23 | Train loss: 0.560 | Val loss: 1.082 | Gen: ethay ayiway onidingincray isway odigray\n",
      "Epoch:  24 | Train loss: 0.538 | Val loss: 1.101 | Gen: ethay airway oninicinedpay isway odigray\n",
      "Epoch:  25 | Train loss: 0.520 | Val loss: 1.033 | Gen: hetway airway ondiniceditay isway ordigway\n",
      "Epoch:  26 | Train loss: 0.505 | Val loss: 1.020 | Gen: ethay airway onidingentcay isway ordigway\n",
      "Epoch:  27 | Train loss: 0.497 | Val loss: 1.046 | Gen: ethay ayiay ondinictionsway isway ordigway\n",
      "Epoch:  28 | Train loss: 0.489 | Val loss: 1.009 | Gen: ethay airway ondinicedtionway isway ordigbay\n",
      "Epoch:  29 | Train loss: 0.479 | Val loss: 1.023 | Gen: ethay airway ondinictionway isway origdway\n",
      "Epoch:  30 | Train loss: 0.469 | Val loss: 0.984 | Gen: ethay airway oninicinedpay isway oringway\n",
      "Epoch:  31 | Train loss: 0.451 | Val loss: 0.944 | Gen: ethay airway onidincionschay isway origdway\n",
      "Epoch:  32 | Train loss: 0.433 | Val loss: 0.944 | Gen: ethay airway ondinicedtishay isway origdway\n",
      "Epoch:  33 | Train loss: 0.419 | Val loss: 0.951 | Gen: ethay airway odininedtchiay isway oridghway\n",
      "Epoch:  34 | Train loss: 0.407 | Val loss: 0.926 | Gen: ethay airway ondinicedtionsay isway oringbway\n",
      "Epoch:  35 | Train loss: 0.413 | Val loss: 1.028 | Gen: ethay airway ondinicedthay isway origdway\n",
      "Epoch:  36 | Train loss: 0.426 | Val loss: 0.964 | Gen: ethay airway ondinicentibay isway ordigwhay\n",
      "Epoch:  37 | Train loss: 0.408 | Val loss: 1.096 | Gen: ehay airway ondinicedtionay isway origdhway\n",
      "Epoch:  38 | Train loss: 0.406 | Val loss: 0.999 | Gen: ethay airway ondinicedithay isway origdhway\n",
      "Epoch:  39 | Train loss: 0.391 | Val loss: 0.987 | Gen: ethay airway odincinedtay isway ordighway\n",
      "Epoch:  40 | Train loss: 0.378 | Val loss: 0.977 | Gen: ethay airway ondicionsingcay isway orkingway\n",
      "Epoch:  41 | Train loss: 0.380 | Val loss: 1.013 | Gen: ethay airway ondinicentibay isway origdhway\n",
      "Epoch:  42 | Train loss: 0.373 | Val loss: 0.970 | Gen: ethay airway ondinicedithay isway ordighway\n",
      "Epoch:  43 | Train loss: 0.353 | Val loss: 0.968 | Gen: ethay airway ondinicetionway isway origdhway\n",
      "Epoch:  44 | Train loss: 0.355 | Val loss: 1.022 | Gen: ethay airway ondinicedtionsay isway oringsway\n",
      "Validation loss has not improved in 10 epochs, stopping early\n",
      "Obtained lowest validation loss of: 0.9261981939825301\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay airway ondicinedtionay isway ordighway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "\n",
    "rnn_args_l = AttrDict()\n",
    "args_dict = {\n",
    "              'data_file_name': 'pig_latin_large',\n",
    "              'cuda':False,\n",
    "              'nepochs':50,\n",
    "              'checkpoint_dir':\"checkpoints\",\n",
    "              'learning_rate':0.005,\n",
    "              'lr_decay':0.99,\n",
    "              'early_stopping_patience':10,\n",
    "              'batch_size':512,\n",
    "              'hidden_size':32,\n",
    "              'encoder_type': 'rnn', # options: rnn / transformer\n",
    "              'decoder_type': 'rnn', # options: rnn / rnn_attention / transformer\n",
    "              'attention_type': '',  # options: additive / scaled_dot\n",
    "}\n",
    "rnn_args_l.update(args_dict)\n",
    "\n",
    "print_opts(rnn_args_l)\n",
    "rnn_encode_l, rnn_decoder_l, rnn_losses_l = train(rnn_args_l)\n",
    "\n",
    "translated = translate_sentence(TEST_SENTENCE, rnn_encode_l, rnn_decoder_l, None, rnn_args_l)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01HsZ6EItc56"
   },
   "source": [
    "The code below plots the training and validation losses of each model, as a function of the number of gradient descent iterations. Consider if there are significant differences in the validation performance of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Qyk_9-Fwtekj",
    "outputId": "e87c56fe-1897-4615-cc5b-db2076452a2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_loss_comparison_lstm(rnn_losses_s, rnn_losses_l, rnn_args_s, rnn_args_l, 'lstm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cE4ijaCzneAt"
   },
   "source": [
    "Select best performing model, and try translating different sentences by changing the variable TEST_SENTENCE. Identify a failure mode and briefly describe it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WrNnz8W1nULf",
    "outputId": "8a517f53-88e8-4772-b421-64a421f5ee48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t\ti am trying test the program \n",
      "translated:\tiway amway yingray esttay ethay opragedcay\n"
     ]
    }
   ],
   "source": [
    "best_encoder = rnn_encode_l # Replace with rnn_losses_s or rnn_losses l\n",
    "best_decoder = rnn_decoder_l # etc.\n",
    "best_args = rnn_args_l\n",
    "\n",
    "TEST_SENTENCE = 'i am trying test the program'\n",
    "translated = translate_sentence(TEST_SENTENCE, best_encoder, best_decoder, None, best_args)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWwA6OGqlaTq"
   },
   "source": [
    "# Part 2: Additive Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJSafHSAmu_w"
   },
   "source": [
    "## Step 1: Additive Attention\n",
    "Already implemented the additive attention mechanism. Write down the mathematical expression for $\\tilde{\\alpha}_i^{(t)}, \\alpha_i^{(t)}, c_t$ as a function of $W_1, W_2, b_1, b_2, Q_t, K_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "AdewEVSMo5jJ"
   },
   "outputs": [],
   "source": [
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(AdditiveAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # A two layer fully-connected network\n",
    "        # hidden_size*2 --> hidden_size, ReLU, hidden_size --> 1\n",
    "        self.attention_network = nn.Sequential(\n",
    "                                    nn.Linear(hidden_size*2, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(hidden_size, 1)\n",
    "                                 )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the additive attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state. (batch_size x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x 1 x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
    "\n",
    "            The attention_weights must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "        batch_size = keys.size(0)\n",
    "        expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(keys)\n",
    "        concat_inputs = torch.cat([expanded_queries, keys], dim=2)\n",
    "        unnormalized_attention = self.attention_network(concat_inputs)\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(attention_weights.transpose(2,1), values)\n",
    "        return context, attention_weights\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73_p8d5EmvOJ"
   },
   "source": [
    "## Step 2: RNN Additive Attention Decoder\n",
    "We will now implement a recurrent decoder that makes use of the additive attention mechanism. Read the description in the assignment worksheet and complete the following implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RJaABkXrpJSw"
   },
   "outputs": [],
   "source": [
    "class RNNAttentionDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, attention_type='scaled_dot'):\n",
    "        super(RNNAttentionDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        self.rnn = MyLSTMCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
    "        if attention_type == 'additive':\n",
    "          self.attention = AdditiveAttention(hidden_size=hidden_size)\n",
    "        elif attention_type == 'scaled_dot':\n",
    "          self.attention = ScaledDotAttention(hidden_size=hidden_size)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
    "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
    "            annotations: The encoder hidden states for each step of the input.\n",
    "                         sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
    "            cell_init: The final cell states from the encoder, across a batch. (batch_size x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
    "\n",
    "        hiddens = []\n",
    "        attentions = []\n",
    "        h_prev = hidden_init\n",
    "        c_prev = cell_init\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            embed_current = embed[:,i,:]  # Get the current time step, across the whole batch\n",
    "            context, attention_weights = self.attention(h_prev, annotations, annotations)  # batch_size x 1 x hidden_size\n",
    "            embed_and_context = torch.cat([embed_current, context.squeeze(1)], dim=1)  # batch_size x (2*hidden_size)\n",
    "            h_prev, c_prev = self.rnn(embed_and_context, h_prev, c_prev)  # batch_size x hidden_size            \n",
    "            \n",
    "            \n",
    "            hiddens.append(h_prev)\n",
    "            attentions.append(attention_weights)\n",
    "\n",
    "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
    "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
    "        \n",
    "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
    "        return output, attentions\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYPae08Io1Fi"
   },
   "source": [
    "## Step 3: Training and Analysis\n",
    "Train the following language model that uses a recurrent encoder, and a recurrent decoder that has an additive attention component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ke6t6rCezpZV",
    "outputId": "6bb1aaea-e84a-4e20-e765-191342352841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_small                        \n",
      "                                   cuda: 0                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.005                                  \n",
      "                               lr_decay: 0.99                                   \n",
      "                early_stopping_patience: 10                                     \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 64                                     \n",
      "                           encoder_type: rnn                                    \n",
      "                           decoder_type: rnn_attention                          \n",
      "                         attention_type: additive                               \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('fears', 'earsfay')\n",
      "('utility', 'utilityway')\n",
      "('accusation', 'accusationway')\n",
      "('i', 'iway')\n",
      "('delirium', 'eliriumday')\n",
      "Num unique word pairs: 3198\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Epoch:   0 | Train loss: 2.108 | Val loss: 1.805 | Gen: ersay ingsay oncingsay inssay oncay-ontay\n",
      "Epoch:   1 | Train loss: 1.546 | Val loss: 1.538 | Gen: etay arway ontiontiontay isway ontay-ontay\n",
      "Epoch:   2 | Train loss: 1.268 | Val loss: 1.354 | Gen: etsay away-iway inconay-ontiontay isway onay-onay-onay\n",
      "Epoch:   3 | Train loss: 1.032 | Val loss: 1.371 | Gen: ay-uthay away inconsingway iway oray-ingway\n",
      "Epoch:   4 | Train loss: 0.870 | Val loss: 1.084 | Gen: ethay arway oncioncionteway isway olway-ingway\n",
      "Epoch:   5 | Train loss: 0.723 | Val loss: 0.996 | Gen: ehtay arway oncincioneday isway orknay-ingway\n",
      "Epoch:   6 | Train loss: 0.601 | Val loss: 0.865 | Gen: ehay away onditingcay-ay-ay isway orway-ingway\n",
      "Epoch:   7 | Train loss: 0.477 | Val loss: 0.777 | Gen: ehtay airway ondintioncionway ishay orknay-ingway\n",
      "Epoch:   8 | Train loss: 0.374 | Val loss: 0.687 | Gen: ehay aiway onditingiontay isway orway-ingway\n",
      "Epoch:   9 | Train loss: 0.301 | Val loss: 0.614 | Gen: ehay aiway onditiontingway isway orkingway\n",
      "Epoch:  10 | Train loss: 0.269 | Val loss: 0.691 | Gen: ehay aiwray onditiontionway isway orklay-ingway\n",
      "Epoch:  11 | Train loss: 0.264 | Val loss: 0.685 | Gen: ethay airway odnitionditnay isway orkingway\n",
      "Epoch:  12 | Train loss: 0.235 | Val loss: 0.656 | Gen: ehay aiway ondititingsay isway orkingway\n",
      "Epoch:  13 | Train loss: 0.192 | Val loss: 0.505 | Gen: ethay airway onditinitingway isway orkingway\n",
      "Epoch:  14 | Train loss: 0.130 | Val loss: 0.448 | Gen: ehtay airway ondititingingay isway orkingway\n",
      "Epoch:  15 | Train loss: 0.096 | Val loss: 0.430 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  16 | Train loss: 0.078 | Val loss: 0.388 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  17 | Train loss: 0.060 | Val loss: 0.429 | Gen: ethay airway onditioningcay isway orkingway\n",
      "Epoch:  18 | Train loss: 0.057 | Val loss: 0.429 | Gen: ethay aiway onditiongingay isway orkingway\n",
      "Epoch:  19 | Train loss: 0.049 | Val loss: 0.428 | Gen: ethay airway onditiongingway isway orkingway\n",
      "Epoch:  20 | Train loss: 0.042 | Val loss: 0.400 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  21 | Train loss: 0.031 | Val loss: 0.346 | Gen: ethay airway onditiongingway isway orkingway\n",
      "Epoch:  22 | Train loss: 0.023 | Val loss: 0.329 | Gen: ethay airway onditiongingway isway orkingway\n",
      "Epoch:  23 | Train loss: 0.021 | Val loss: 0.341 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  24 | Train loss: 0.018 | Val loss: 0.325 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  25 | Train loss: 0.016 | Val loss: 0.316 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  26 | Train loss: 0.014 | Val loss: 0.335 | Gen: ethay airway onditiongingway isway orkingway\n",
      "Epoch:  27 | Train loss: 0.011 | Val loss: 0.329 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  28 | Train loss: 0.009 | Val loss: 0.309 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  29 | Train loss: 0.007 | Val loss: 0.294 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  30 | Train loss: 0.006 | Val loss: 0.289 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  31 | Train loss: 0.005 | Val loss: 0.287 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  32 | Train loss: 0.005 | Val loss: 0.285 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  33 | Train loss: 0.004 | Val loss: 0.285 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  34 | Train loss: 0.004 | Val loss: 0.283 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  35 | Train loss: 0.004 | Val loss: 0.282 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  36 | Train loss: 0.003 | Val loss: 0.282 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  37 | Train loss: 0.003 | Val loss: 0.281 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  38 | Train loss: 0.003 | Val loss: 0.282 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  39 | Train loss: 0.003 | Val loss: 0.282 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  40 | Train loss: 0.002 | Val loss: 0.282 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  41 | Train loss: 0.002 | Val loss: 0.282 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  42 | Train loss: 0.002 | Val loss: 0.283 | Gen: ethay airway onditiongingcay isway orkingway\n",
      "Epoch:  43 | Train loss: 0.126 | Val loss: 1.333 | Gen: ethay aiy-areway ondnitiongionway isway orwongiblway\n",
      "Epoch:  44 | Train loss: 0.631 | Val loss: 0.741 | Gen: ethay aiy indiontiongday isway owlay-ingfay\n",
      "Epoch:  45 | Train loss: 0.344 | Val loss: 0.559 | Gen: ethay airway onditiongday isway orkingway\n",
      "Epoch:  46 | Train loss: 0.154 | Val loss: 0.485 | Gen: ehay airway ondititiongday isway orkingway\n",
      "Epoch:  47 | Train loss: 0.094 | Val loss: 0.425 | Gen: ethay airway onditiongingway isway orkingway\n",
      "Validation loss has not improved in 10 epochs, stopping early\n",
      "Obtained lowest validation loss of: 0.2814960081084836\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tethay airway onditioningcay isway orkingway\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "\n",
    "rnn_attn_args = AttrDict()\n",
    "args_dict = {\n",
    "              'data_file_name': 'pig_latin_small',\n",
    "              'cuda':False, \n",
    "              'nepochs':50, \n",
    "              'checkpoint_dir':\"checkpoints\", \n",
    "              'learning_rate':0.005,\n",
    "              'lr_decay':0.99,\n",
    "              'early_stopping_patience':10,\n",
    "              'batch_size':64, \n",
    "              'hidden_size':64, \n",
    "              'encoder_type': 'rnn', # options: rnn / transformer\n",
    "              'decoder_type': 'rnn_attention', # options: rnn / rnn_attention / transformer\n",
    "              'attention_type': 'additive',  # options: additive / scaled_dot\n",
    "}\n",
    "rnn_attn_args.update(args_dict)\n",
    "\n",
    "print_opts(rnn_attn_args)\n",
    "rnn_attn_encoder, rnn_attn_decoder, rnn_attn_losses = train(rnn_attn_args)\n",
    "\n",
    "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNVKbLc0ACj_",
    "outputId": "2c93b164-8fa7-46d5-a1ea-f2d9724a4762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t\ti am trying test the program \n",
      "translated:\tiway amway yingtray esttay ethay ogrampray\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'i am trying test the program'\n",
    "translated = translate_sentence(TEST_SENTENCE, rnn_attn_encoder, rnn_attn_decoder, None, rnn_attn_args)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kw_GOIvzo1ix"
   },
   "source": [
    "# Part 3: Scaled Dot Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xq7nhsEio1w-"
   },
   "source": [
    "## Step 1: Implement Dot-Product Attention\n",
    "Implement the scaled dot product attention module described in the assignment worksheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "d_j3oY3hqsJQ"
   },
   "outputs": [],
   "source": [
    "class ScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
    "\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # FILL THIS IN\n",
    "        # ------------\n",
    "        batch_size = queries.shape[0]\n",
    "        q = self.Q(queries)\n",
    "        k = self.K(keys)\n",
    "        v = self.V(values)\n",
    "        # print(q.transpose(2,1).shape)\n",
    "        unnormalized_attention = torch.bmm(k, q.transpose(2,1)) / self.scaling_factor\n",
    "        attention_weights = self.softmax(unnormalized_attention)\n",
    "        context = torch.bmm(attention_weights.transpose(2,1), v)\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unReAOrjo113"
   },
   "source": [
    "## Step 2: Implement Causal Dot-Product Attention\n",
    "Now implement the scaled causal dot product described in the assignment worksheet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ovigzQffrKqj"
   },
   "outputs": [],
   "source": [
    "class CausalScaledDotAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(CausalScaledDotAttention, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.neg_inf = torch.tensor(-1e7)\n",
    "\n",
    "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"The forward pass of the scaled dot attention mechanism.\n",
    "\n",
    "        Arguments:\n",
    "            queries: The current decoder hidden state, 2D or 3D tensor. (batch_size x (k) x hidden_size)\n",
    "            keys: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            values: The encoder hidden states for each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "\n",
    "        Returns:\n",
    "            context: weighted average of the values (batch_size x k x hidden_size)\n",
    "            attention_weights: Normalized attention weights for each encoder hidden state. (batch_size x seq_len x 1)\n",
    "\n",
    "            The output must be a softmax weighting over the seq_len annotations.\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------\n",
    "        # FILL THIS IN\n",
    "        # ------------\n",
    "        batch_size = queries.shape[0]\n",
    "        q = self.Q(queries)\n",
    "        k = self.K(keys)\n",
    "        v = self.V(values)\n",
    "        unnormalized_attention = torch.bmm(k, q.transpose(2,1)) / self.scaling_factor\n",
    "        mask = torch.tril(torch.ones_like(unnormalized_attention))*self.neg_inf\n",
    "        attention_weights = self.softmax(mask+ unnormalized_attention)\n",
    "        context = torch.bmm(attention_weights.transpose(2,1), v)\n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tcpUFKqo2Oi"
   },
   "source": [
    "## Step 3: Transformer Encoder\n",
    "Complete the following transformer encoder implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "N3B-fWsarlVk"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, opts):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.opts = opts\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        \n",
    "        self.self_attentions = nn.ModuleList([ScaledDotAttention(\n",
    "                                    hidden_size=hidden_size, \n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
    "                                    nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                 ) for i in range(self.num_layers)])\n",
    "\n",
    "        self.positional_encodings = self.create_positional_encodings()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass of the encoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
    "\n",
    "        Returns:\n",
    "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
    "            None: Used to conform to standard encoder return signature.\n",
    "            None: Used to conform to standard encoder return signature.        \n",
    "        \"\"\"\n",
    "        batch_size, seq_len = inputs.size()\n",
    "\n",
    "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        # Add positinal embeddings from self.create_positional_encodings. (a'la https://arxiv.org/pdf/1706.03762.pdf, section 3.5)\n",
    "        encoded = encoded + self.positional_encodings[:seq_len]\n",
    "\n",
    "        annotations = encoded\n",
    "        for i in range(self.num_layers):\n",
    "            new_annotations, self_attention_weights = self.self_attentions[i](annotations, annotations, annotations)  # batch_size x seq_len x hidden_size\n",
    "            residual_annotations = annotations + new_annotations\n",
    "            new_annotations = self.attention_mlps[i](residual_annotations)\n",
    "            annotations = residual_annotations + new_annotations\n",
    "\n",
    "        # Transformer encoder does not have a last hidden or cell layer. \n",
    "        return annotations, None, None\n",
    "\n",
    "    def create_positional_encodings(self, max_seq_len=1000):\n",
    "        \"\"\"Creates positional encodings for the inputs.\n",
    "\n",
    "        Arguments:\n",
    "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
    "\n",
    "        Returns:\n",
    "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
    "        \"\"\"\n",
    "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
    "        dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
    "        exponents = (2*dim_indices).float()/(self.hidden_size)\n",
    "        trig_args = pos_indices / (10000**exponents)\n",
    "        sin_terms = torch.sin(trig_args)\n",
    "        cos_terms = torch.cos(trig_args)\n",
    "\n",
    "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
    "        pos_encodings[:, 0::2] = sin_terms\n",
    "        pos_encodings[:, 1::2] = cos_terms\n",
    "\n",
    "        if self.opts.cuda:\n",
    "            pos_encodings = pos_encodings.cuda()\n",
    "\n",
    "        return pos_encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1hDi020rT36"
   },
   "source": [
    "## Step 4: Transformer Decoder\n",
    "Complete the following transformer decoder implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "nyvTZFxtrvc6"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.self_attentions = nn.ModuleList([CausalScaledDotAttention(\n",
    "                                    hidden_size=hidden_size, \n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.encoder_attentions = nn.ModuleList([ScaledDotAttention(\n",
    "                                    hidden_size=hidden_size, \n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.attention_mlps = nn.ModuleList([nn.Sequential(\n",
    "                                    nn.Linear(hidden_size, hidden_size),\n",
    "                                    nn.ReLU(),\n",
    "                                 ) for i in range(self.num_layers)])\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        self.positional_encodings = self.create_positional_encodings()\n",
    "\n",
    "    def forward(self, inputs, annotations, hidden_init, cell_init):\n",
    "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
    "\n",
    "        Arguments:\n",
    "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
    "            annotations: The encoder hidden states for each step of the input.\n",
    "                         sequence. (batch_size x seq_len x hidden_size)\n",
    "            hidden_init: Not used in the transformer decoder\n",
    "            cell_init: Not used in transformer decoder\n",
    "        Returns:\n",
    "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
    "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size, seq_len = inputs.size()\n",
    "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
    "\n",
    "        embed = embed + self.positional_encodings[:seq_len]\n",
    "\n",
    "        encoder_attention_weights_list = []\n",
    "        self_attention_weights_list = []\n",
    "        contexts = embed\n",
    "        for i in range(self.num_layers):\n",
    "            new_contexts, self_attention_weights = self.self_attentions[i](contexts, contexts, contexts)  # batch_size x seq_len x hidden_size\n",
    "            residual_contexts = contexts + new_contexts\n",
    "            new_contexts, encoder_attention_weights = self.encoder_attentions[i](residual_contexts, annotations, annotations) # batch_size x seq_len x hidden_size\n",
    "            residual_contexts = residual_contexts + new_contexts\n",
    "            new_contexts = self.attention_mlps[i](residual_contexts)\n",
    "            contexts = residual_contexts + new_contexts\n",
    "\n",
    "            encoder_attention_weights_list.append(encoder_attention_weights)\n",
    "            self_attention_weights_list.append(self_attention_weights)\n",
    "          \n",
    "        output = self.out(contexts)\n",
    "        encoder_attention_weights = torch.stack(encoder_attention_weights_list)\n",
    "        self_attention_weights = torch.stack(self_attention_weights_list)\n",
    "        \n",
    "        return output, (encoder_attention_weights, self_attention_weights)\n",
    "\n",
    "    def create_positional_encodings(self, max_seq_len=1000):\n",
    "        \"\"\"Creates positional encodings for the inputs.\n",
    "\n",
    "        Arguments:\n",
    "            max_seq_len: a number larger than the maximum string length we expect to encounter during training\n",
    "\n",
    "        Returns:\n",
    "            pos_encodings: (max_seq_len, hidden_dim) Positional encodings for a sequence with length max_seq_len. \n",
    "        \"\"\"\n",
    "        pos_indices = torch.arange(max_seq_len)[..., None]\n",
    "        dim_indices = torch.arange(self.hidden_size//2)[None, ...]\n",
    "        exponents = (2*dim_indices).float()/(self.hidden_size)\n",
    "        trig_args = pos_indices / (10000**exponents)\n",
    "        sin_terms = torch.sin(trig_args)\n",
    "        cos_terms = torch.cos(trig_args)\n",
    "\n",
    "        pos_encodings = torch.zeros((max_seq_len, self.hidden_size))\n",
    "        pos_encodings[:, 0::2] = sin_terms\n",
    "        pos_encodings[:, 1::2] = cos_terms\n",
    "\n",
    "        #pos_encodings = pos_encodings.cuda()\n",
    "        pos_encodings = pos_encodings\n",
    "        \n",
    "        return pos_encodings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29ZjkXTNrUKb"
   },
   "source": [
    "\n",
    "## Step 5: Training and analysis\n",
    "Now, train the following language model that's comprised of a (simplified) transformer encoder and transformer decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqTp-eCPuuFO"
   },
   "source": [
    "First, we train our smaller model on the small dataset. Use this model to answer Question 4 in the handout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "mk8e4KSnuZ8N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_small                        \n",
      "                                   cuda: 0                                      \n",
      "                                nepochs: 100                                    \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.0005                                 \n",
      "                early_stopping_patience: 100                                    \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 32                                     \n",
      "                           encoder_type: transformer                            \n",
      "                           decoder_type: transformer                            \n",
      "                 num_transformer_layers: 3                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('fears', 'earsfay')\n",
      "('utility', 'utilityway')\n",
      "('accusation', 'accusationway')\n",
      "('i', 'iway')\n",
      "('delirium', 'eliriumday')\n",
      "Num unique word pairs: 3198\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Epoch:   0 | Train loss: 3.942 | Val loss: 3.340 | Gen: odeee antacnwstyyadttanana EOSlu EOSyyyyyyyyyyyyyyyyyyy ln\n",
      "Epoch:   1 | Train loss: 3.132 | Val loss: 2.892 | Gen: ooy eattttaasssayyEOSEOSEOSnnw lr ayyyyyyyyyyyyyyyy ottaewyyyywwwwwywyyy\n",
      "Epoch:   2 | Train loss: 2.783 | Val loss: 2.713 | Gen: ehay aytrraypynny irayydyyEOSyyyyyyai ihay oeyyywy\n",
      "Epoch:   3 | Train loss: 2.602 | Val loss: 2.563 | Gen: ehay artay iryyEOSrryyyy ilay wayEOSy\n",
      "Epoch:   4 | Train loss: 2.486 | Val loss: 2.480 | Gen: eaay aray iay-ayEOSay-ay-ayy---- aywayyyyyyyyyyyyyyyy awwyyaatgwwwy\n",
      "Epoch:   5 | Train loss: 2.404 | Val loss: 2.388 | Gen: oay away ay ayway anwy\n",
      "Epoch:   6 | Train loss: 2.334 | Val loss: 2.307 | Gen: eay away ay-y ay iway\n",
      "Epoch:   7 | Train loss: 2.245 | Val loss: 2.284 | Gen: enehay away iyliwy-iiiiiiiEOSay iay inmEOSywy\n",
      "Epoch:   8 | Train loss: 2.189 | Val loss: 2.235 | Gen: oahy away innldiiiyEOSay iway owwy\n",
      "Epoch:   9 | Train loss: 2.154 | Val loss: 2.200 | Gen: aeyy away inslynidEOSiayy iwayy nnggy\n",
      "Epoch:  10 | Train loss: 2.107 | Val loss: 2.158 | Gen: ewy away inggnygny way onny\n",
      "Epoch:  11 | Train loss: 2.069 | Val loss: 2.120 | Gen: eay away ansiaday iwaway onanay\n",
      "Epoch:  12 | Train loss: 2.037 | Val loss: 2.150 | Gen: ehay ay ingiay iway iray\n",
      "Epoch:  13 | Train loss: 2.025 | Val loss: 2.108 | Gen: eay ay angiagdy iway onnyEOSndayEOSayEOSy\n",
      "Epoch:  14 | Train loss: 1.992 | Val loss: 2.103 | Gen: ohgy away iiniynay iway orway\n",
      "Epoch:  15 | Train loss: 1.958 | Val loss: 2.086 | Gen: eay away ingay iwayy onsynEOSnoniEOSy\n",
      "Epoch:  16 | Train loss: 1.961 | Val loss: 2.090 | Gen: ehay away iygay iway ingway\n",
      "Epoch:  17 | Train loss: 1.952 | Val loss: 2.055 | Gen: eay ay iigay iwaay angany\n",
      "Epoch:  18 | Train loss: 1.935 | Val loss: 2.031 | Gen: any ay ainay iwaay innaay\n",
      "Epoch:  19 | Train loss: 1.924 | Val loss: 2.071 | Gen: ay away ingay iway aayy\n",
      "Epoch:  20 | Train loss: 1.940 | Val loss: 2.032 | Gen: ay ay ingayEOSyyy iwaay ay\n",
      "Epoch:  21 | Train loss: 1.904 | Val loss: 2.012 | Gen: ay ay ingay iwaay away\n",
      "Epoch:  22 | Train loss: 1.878 | Val loss: 1.987 | Gen: ay ay ingay iwway oy\n",
      "Epoch:  23 | Train loss: 1.854 | Val loss: 1.978 | Gen: ay ay ingy ay ay    \n",
      "Epoch:  24 | Train loss: 1.831 | Val loss: 1.934 | Gen: ay ay ingaaay iy angay\n",
      "Epoch:  25 | Train loss: 1.819 | Val loss: 1.941 | Gen: ay ay ingay iway angway\n",
      "Epoch:  26 | Train loss: 1.815 | Val loss: 1.939 | Gen: ay ay inday ay owgaay\n",
      "Epoch:  27 | Train loss: 1.809 | Val loss: 1.916 | Gen: ay ay ingiiiiyiiiiingayy-i ay awaway\n",
      "Epoch:  28 | Train loss: 1.784 | Val loss: 1.926 | Gen: ay ay ongay-yy ay angway\n",
      "Epoch:  29 | Train loss: 1.767 | Val loss: 1.881 | Gen: ay ay ongioy-yoy ay awaway\n",
      "Epoch:  30 | Train loss: 1.753 | Val loss: 1.879 | Gen: ay ay onggnginginganyy iway awgway\n",
      "Epoch:  31 | Train loss: 1.730 | Val loss: 1.875 | Gen: ay ay iydaaoooy iway awaway\n",
      "Epoch:  32 | Train loss: 1.713 | Val loss: 1.869 | Gen: ay ay inday iway awgway\n",
      "Epoch:  33 | Train loss: 1.694 | Val loss: 1.863 | Gen: ay ay iiiaaaoiiiiiinyyyy iway angay\n",
      "Epoch:  34 | Train loss: 1.675 | Val loss: 1.856 | Gen: ay ay ingingingingay iway angway\n",
      "Epoch:  35 | Train loss: 1.671 | Val loss: 1.876 | Gen: ay ay intinday iway anwway\n",
      "Epoch:  36 | Train loss: 1.675 | Val loss: 1.870 | Gen: ay ay indway iway ongway\n",
      "Epoch:  37 | Train loss: 1.658 | Val loss: 1.864 | Gen: ay ay ingay ay angway\n",
      "Epoch:  38 | Train loss: 1.686 | Val loss: 1.891 | Gen: eay ay inddiiiiiiingay iy ongononngway\n",
      "Epoch:  39 | Train loss: 1.671 | Val loss: 1.854 | Gen: eay ay ingingay insay ongwaaagwaay\n",
      "Epoch:  40 | Train loss: 1.649 | Val loss: 1.826 | Gen: ey ay ongay-EOSy iy ongway\n",
      "Epoch:  41 | Train loss: 1.622 | Val loss: 1.830 | Gen: ay ay angay iy angway\n",
      "Epoch:  42 | Train loss: 1.613 | Val loss: 1.840 | Gen: ay ay onconcayEOSooooonnoooo iy angway\n",
      "Epoch:  43 | Train loss: 1.629 | Val loss: 1.839 | Gen: ay ay onsay iy orgray\n",
      "Epoch:  44 | Train loss: 1.617 | Val loss: 1.799 | Gen: away ay onsay ay ongway\n",
      "Epoch:  45 | Train loss: 1.594 | Val loss: 1.811 | Gen: ay ay ontinsay ay orray\n",
      "Epoch:  46 | Train loss: 1.587 | Val loss: 1.813 | Gen: ay ay ontay iy ongay\n",
      "Epoch:  47 | Train loss: 1.584 | Val loss: 1.798 | Gen: ay ay antingay anay angway\n",
      "Epoch:  48 | Train loss: 1.567 | Val loss: 1.792 | Gen: ay ay ontay anaay anwway\n",
      "Epoch:  49 | Train loss: 1.560 | Val loss: 1.783 | Gen: ay ay ongayEOSooooonay awaay ongwaaEOSay\n",
      "Epoch:  50 | Train loss: 1.551 | Val loss: 1.771 | Gen: ay ay ontinaay ay ongray\n",
      "Epoch:  51 | Train loss: 1.540 | Val loss: 1.752 | Gen: ay ay oy ay arkray  \n",
      "Epoch:  52 | Train loss: 1.529 | Val loss: 1.744 | Gen: ay ay onty ay ongray\n",
      "Epoch:  53 | Train loss: 1.511 | Val loss: 1.738 | Gen: ay ay onganay asyay onarayEOSy\n",
      "Epoch:  54 | Train loss: 1.510 | Val loss: 1.734 | Gen: ay ay ontonsaodaaaaayy ay ongngongay\n",
      "Epoch:  55 | Train loss: 1.494 | Val loss: 1.725 | Gen: ay ay onty ay ongonggay\n",
      "Epoch:  56 | Train loss: 1.475 | Val loss: 1.716 | Gen: ay ay inty ay ingonggay\n",
      "Epoch:  57 | Train loss: 1.468 | Val loss: 1.722 | Gen: ay ay inssiy ay ongngongay\n",
      "Epoch:  58 | Train loss: 1.480 | Val loss: 1.715 | Gen: ay ay intinay issyEOSEOSy ongongay\n",
      "Epoch:  59 | Train loss: 1.467 | Val loss: 1.717 | Gen: ay ay onstay iayyyEOSay ongay\n",
      "Epoch:  60 | Train loss: 1.460 | Val loss: 1.698 | Gen: ay ay oncaanEOSyyy ay ongongay\n",
      "Epoch:  61 | Train loss: 1.446 | Val loss: 1.715 | Gen: ay ay insinay ay ongwanEOSay\n",
      "Epoch:  62 | Train loss: 1.426 | Val loss: 1.715 | Gen: ay awaay oncayEOSoooyy asyay ongingay\n",
      "Epoch:  63 | Train loss: 1.423 | Val loss: 1.687 | Gen: ay ayay ongonsty asyay ingingingingay\n",
      "Epoch:  64 | Train loss: 1.414 | Val loss: 1.715 | Gen: cy ay ongayEOSy ay ingranrygry\n",
      "Epoch:  65 | Train loss: 1.409 | Val loss: 1.698 | Gen: ay awaay intiny iy ingingay\n",
      "Epoch:  66 | Train loss: 1.417 | Val loss: 1.687 | Gen: cy awaay imaay aysyy ingingay\n",
      "Epoch:  67 | Train loss: 1.410 | Val loss: 1.669 | Gen: cy ayay inday iy ingray\n",
      "Epoch:  68 | Train loss: 1.408 | Val loss: 1.677 | Gen: ay ayay intinay iyyEOSy ingingonggnggy\n",
      "Epoch:  69 | Train loss: 1.400 | Val loss: 1.684 | Gen: eay araay ongingiygiyyayEOSaaEOSoo iaasy ongingangay\n",
      "Epoch:  70 | Train loss: 1.429 | Val loss: 1.664 | Gen: eay aray onsty asayy angoaaahayay\n",
      "Epoch:  71 | Train loss: 1.398 | Val loss: 1.660 | Gen: ey aray onaibay iy angoaghaghay\n",
      "Epoch:  72 | Train loss: 1.384 | Val loss: 1.639 | Gen: eay aray ocdicooy iy angongonggingay\n",
      "Epoch:  73 | Train loss: 1.368 | Val loss: 1.682 | Gen: eay aray oddayEOSoy isay ongrngonyEOSEOSEOSEOSEOSngy\n",
      "Epoch:  74 | Train loss: 1.389 | Val loss: 1.652 | Gen: eay aray ontiotttayyEOSoyEOSy issay inggngogggggggy\n",
      "Epoch:  75 | Train loss: 1.374 | Val loss: 1.684 | Gen: eay aray ontionay issay onggongay\n",
      "Epoch:  76 | Train loss: 1.364 | Val loss: 1.660 | Gen: eay aray ontinay issay amay\n",
      "Epoch:  77 | Train loss: 1.366 | Val loss: 1.648 | Gen: eay aray ontionay issay ongongay\n",
      "Epoch:  78 | Train loss: 1.345 | Val loss: 1.647 | Gen: ay aray ontiontay issay ay\n",
      "Epoch:  79 | Train loss: 1.356 | Val loss: 1.641 | Gen: jy aray oddayEOSEOSy issay ongonggay\n",
      "Epoch:  80 | Train loss: 1.336 | Val loss: 1.635 | Gen: ey aray ontoonnatsnny issyy inghnghighay\n",
      "Epoch:  81 | Train loss: 1.331 | Val loss: 1.649 | Gen: ey aray ongongoony issssyy ongongongnhnngyEOSoo\n",
      "Epoch:  82 | Train loss: 1.322 | Val loss: 1.634 | Gen: ey aray indiddaodatoyy issay angwaagaaaagyay\n",
      "Epoch:  83 | Train loss: 1.306 | Val loss: 1.637 | Gen: ay aray onttnssay isay angwnnhnaagayay\n",
      "Epoch:  84 | Train loss: 1.326 | Val loss: 1.624 | Gen: ay aray onsinsay issay onginghnghy\n",
      "Epoch:  85 | Train loss: 1.319 | Val loss: 1.632 | Gen: eay ay onsssay isssay onggongay\n",
      "Epoch:  86 | Train loss: 1.323 | Val loss: 1.611 | Gen: ehayeyyyEOSy ay onsinay issay onggongay\n",
      "Epoch:  87 | Train loss: 1.306 | Val loss: 1.604 | Gen: ay aray instionay issay ongonghay\n",
      "Epoch:  88 | Train loss: 1.292 | Val loss: 1.594 | Gen: iay aray instionay isssay ongwanggawEOSaaaaEOSEOSaya\n",
      "Epoch:  89 | Train loss: 1.284 | Val loss: 1.597 | Gen: eay aray onsinsinsinsty isssay onggongay\n",
      "Epoch:  90 | Train loss: 1.274 | Val loss: 1.594 | Gen: eay aray onsinsinsinsty issay ingwaghaawaaaay\n",
      "Epoch:  91 | Train loss: 1.268 | Val loss: 1.580 | Gen: ehaEOSaday aray onsinsinsinsty issay ingwagiaawayway\n",
      "Epoch:  92 | Train loss: 1.256 | Val loss: 1.583 | Gen: eay ay inssnsaassiaaay isssay ingwangaaagay\n",
      "Epoch:  93 | Train loss: 1.249 | Val loss: 1.620 | Gen: eay ay insionay issay ingongwaawaaawy\n",
      "Epoch:  94 | Train loss: 1.331 | Val loss: 1.642 | Gen: eay aray indinday isssssay ingwaawaaway\n",
      "Epoch:  95 | Train loss: 1.286 | Val loss: 1.594 | Gen: eay aray instiaay issssay onwwayyyayyyyyyyEOSwy\n",
      "Epoch:  96 | Train loss: 1.287 | Val loss: 1.685 | Gen: ay aray indiondday isssay ongingghyEOSo\n",
      "Epoch:  97 | Train loss: 1.356 | Val loss: 1.649 | Gen: ay aray oddddiaayay isssay onghhhghiggy\n",
      "Epoch:  98 | Train loss: 1.329 | Val loss: 1.632 | Gen: ay aray oonaybay isssay onghhhghiigy\n",
      "Epoch:  99 | Train loss: 1.297 | Val loss: 1.634 | Gen: ay aray oonaooayaayy isssay onghinginggyyEOSooy\n",
      "Obtained lowest validation loss of: 1.5803557430825583\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tay aray oonaooayaayy isssay onghinginggyyEOSooy\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "\n",
    "trans32_args_s = AttrDict()\n",
    "args_dict = {\n",
    "              'data_file_name': 'pig_latin_small',\n",
    "              'cuda':False, \n",
    "              'nepochs':100, \n",
    "              'checkpoint_dir':\"checkpoints\", \n",
    "              'learning_rate':5e-4,\n",
    "              'early_stopping_patience': 100,\n",
    "              'lr_decay':0.99,\n",
    "              'batch_size': 64,\n",
    "              'hidden_size': 32,\n",
    "              'encoder_type': 'transformer',\n",
    "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
    "              'num_transformer_layers': 3,\n",
    "}\n",
    "trans32_args_s.update(args_dict)\n",
    "print_opts(trans32_args_s)\n",
    "\n",
    "trans32_encoder_s, trans32_decoder_s, trans32_losses_s = train(trans32_args_s)\n",
    "\n",
    "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "l28mKuZxvaRT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:\t\tthe air conditioning is working \n",
      "translated:\tay aray oonaooayaayy isssay onghinginggyyEOSooy\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_s, trans32_decoder_s, None, trans32_args_s)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0L8EqLYFu48H"
   },
   "source": [
    "In the following cells, we investigate the effects of increasing model size and dataset size on the training / validation curves and generalization of the Transformer. We will increase hidden size to 64, and also increase dataset size. Include the best achieved validation loss in your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "FdZO69DozuUu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_large                        \n",
      "                                   cuda: 0                                      \n",
      "                                nepochs: 100                                    \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.0005                                 \n",
      "                early_stopping_patience: 10                                     \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 512                                    \n",
      "                            hidden_size: 32                                     \n",
      "                           encoder_type: transformer                            \n",
      "                           decoder_type: transformer                            \n",
      "                 num_transformer_layers: 3                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('tutor', 'utortay')\n",
      "('es', 'esway')\n",
      "('dams', 'amsday')\n",
      "('gala', 'alagay')\n",
      "('rehearsal', 'ehearsalray')\n",
      "Num unique word pairs: 22402\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Epoch:   0 | Train loss: 3.583 | Val loss: 3.084 | Gen: yyyyyyyyyyyyyyyyyyyy ywwwwyyay eyey yyyyyyyyyyyyyyyyyyyy yoiayayyydaiEOSy\n",
      "Epoch:   1 | Train loss: 2.903 | Val loss: 2.810 | Gen: yyyyyyyyyyyyyyyyyyyy awEOSwaawayyy aacay ay eoeoeoeoeay\n",
      "Epoch:   2 | Train loss: 2.665 | Val loss: 2.601 | Gen: yyyyyyyyyyyyyyyyyyyy ay dhyyEOSy ey eniay\n",
      "Epoch:   3 | Train loss: 2.502 | Val loss: 2.508 | Gen: yyyyyyyyyyyyyyyy ayny elcanty ay unta-asay\n",
      "Epoch:   4 | Train loss: 2.413 | Val loss: 2.443 | Gen: yyyyyyyyyyyyyyyyyyyy ay euat ayy entay\n",
      "Epoch:   5 | Train loss: 2.320 | Val loss: 2.350 | Gen: ytEOSyy oyEOSay il-yy iyEOSayy et-yy\n",
      "Epoch:   6 | Train loss: 2.259 | Val loss: 2.304 | Gen: othtwsy ayyEOSy olaocEOSyEOSy ilayty ontay\n",
      "Epoch:   7 | Train loss: 2.190 | Val loss: 2.261 | Gen: utay ayyEOSy ocaccay ay ow-oliiayaEOSyy\n",
      "Epoch:   8 | Train loss: 2.146 | Val loss: 2.255 | Gen: utay iodayyy eiactyyy ay antyaEOSy\n",
      "Epoch:   9 | Train loss: 2.119 | Val loss: 2.239 | Gen: otay ayty otccay atEOSy enay\n",
      "Epoch:  10 | Train loss: 2.091 | Val loss: 2.205 | Gen: ay onaayyy oucacacacacacacaca ay allwEOSay\n",
      "Epoch:  11 | Train loss: 2.062 | Val loss: 2.179 | Gen: otay intiniyiaay ulllyEOSaaslyEOSEOSlsloa itEOSy itlyyy\n",
      "Epoch:  12 | Train loss: 2.024 | Val loss: 2.138 | Gen: otay-oy-y-y iltay ocloucay ay illwayiaiy\n",
      "Epoch:  13 | Train loss: 2.000 | Val loss: 2.106 | Gen: othy-oyy ionay icay intwyay entway\n",
      "Epoch:  14 | Train loss: 1.967 | Val loss: 2.095 | Gen: otayyytay ioiay oncooooooeay intwny eraaay\n",
      "Epoch:  15 | Train loss: 1.951 | Val loss: 2.087 | Gen: eayy alay ocoynEOSEOSEOSy illay erayyy\n",
      "Epoch:  16 | Train loss: 1.928 | Val loss: 2.093 | Gen: ota--ayy ayy otany ilinay entww-iay\n",
      "Epoch:  17 | Train loss: 1.898 | Val loss: 2.055 | Gen: oyaay-yEOSEOSEOSEOSyEOSy iyEOSy oncayEOSty insway enaay\n",
      "Epoch:  18 | Train loss: 1.885 | Val loss: 2.040 | Gen: oty aayy ocaonoaoy insay enwy\n",
      "Epoch:  19 | Train loss: 1.869 | Val loss: 2.044 | Gen: ay away ocay insay ewaay\n",
      "Epoch:  20 | Train loss: 1.864 | Val loss: 2.040 | Gen: ottay iygyyyy occoyaay insisisilay eday\n",
      "Epoch:  21 | Train loss: 1.869 | Val loss: 2.058 | Gen: otay-oay iniaiEOSay intinioinininininiow insisisisiyay ewgayEOSinway\n",
      "Epoch:  22 | Train loss: 1.848 | Val loss: 1.994 | Gen: otay iggy ineininnysy issay eway\n",
      "Epoch:  23 | Train loss: 1.830 | Val loss: 2.026 | Gen: otay iydyEOSy ootiionoEOSiy iy eway\n",
      "Epoch:  24 | Train loss: 1.816 | Val loss: 2.016 | Gen: otay ilay oootiotyy isay eway\n",
      "Epoch:  25 | Train loss: 1.832 | Val loss: 2.047 | Gen: oyEOSy ilayEOSy otttetoaaayEOSy ay eway-iway\n",
      "Epoch:  26 | Train loss: 1.812 | Val loss: 2.012 | Gen: otay ilayy ointiotteooy insy eway-inwaway\n",
      "Epoch:  27 | Train loss: 1.796 | Val loss: 1.995 | Gen: atay ilay ottceiay isay eway-idway\n",
      "Epoch:  28 | Train loss: 1.770 | Val loss: 2.002 | Gen: oya--y ilay olirny atsyy inwywinway\n",
      "Epoch:  29 | Train loss: 1.769 | Val loss: 1.964 | Gen: ay ilay ournery essy enwy-inaay\n",
      "Epoch:  30 | Train loss: 1.758 | Val loss: 1.979 | Gen: ay ilay oclay aay eray-inway\n",
      "Epoch:  31 | Train loss: 1.759 | Val loss: 1.918 | Gen: ay iway ooay asEOSy ewwy\n",
      "Epoch:  32 | Train loss: 1.738 | Val loss: 1.935 | Gen: ay iy onoiynyyEOSEOSEOSEOSEOSy iyEOSsEOSyy owayEOSinyyy\n",
      "Epoch:  33 | Train loss: 1.779 | Val loss: 2.015 | Gen: atay-ay iy ectcotyyyyayEOSyyaayy ay olla-yEOSns-ooyry\n",
      "Epoch:  34 | Train loss: 1.765 | Val loss: 2.012 | Gen: ay iy innniy ay ewwanwaaaaay\n",
      "Epoch:  35 | Train loss: 1.735 | Val loss: 1.950 | Gen: ay ay oncay ay enwa-anEOSyaEOSy\n",
      "Epoch:  36 | Train loss: 1.716 | Val loss: 1.927 | Gen: ayEOSy iy oncay eyway ongnway\n",
      "Epoch:  37 | Train loss: 1.701 | Val loss: 1.948 | Gen: ty iway intininininincay aysay inway-ay\n",
      "Epoch:  38 | Train loss: 1.730 | Val loss: 1.932 | Gen: aty iy otay eyayy inwy-idEOSayy\n",
      "Epoch:  39 | Train loss: 1.713 | Val loss: 1.929 | Gen: oyEOSy ay ontay isayy ewaawaaway\n",
      "Epoch:  40 | Train loss: 1.693 | Val loss: 1.898 | Gen: ushay ilay oterecayy isayy owaayinwaaay\n",
      "Epoch:  41 | Train loss: 1.678 | Val loss: 1.894 | Gen: elayy ilay outiy iwaay enwayinwayEOSwEOSy\n",
      "Epoch:  42 | Train loss: 1.658 | Val loss: 1.880 | Gen: othay inay ontwayyyyy isayy enwa-owwaaay\n",
      "Epoch:  43 | Train loss: 1.648 | Val loss: 1.907 | Gen: ay ilway intway iwway oway-oway\n",
      "Epoch:  44 | Train loss: 1.679 | Val loss: 1.890 | Gen: ay ilay otirtatay ishay oliggyEOSilay\n",
      "Epoch:  45 | Train loss: 1.651 | Val loss: 1.892 | Gen: ayy inway ontintiontiontioncay inway ellwy-nwaaayEOSy\n",
      "Epoch:  46 | Train loss: 1.639 | Val loss: 1.903 | Gen: ayy inwaay ontiay issay enininiydynynidiy\n",
      "Epoch:  47 | Train loss: 1.616 | Val loss: 1.904 | Gen: ayy anwaay ontiontiontiontay isay ellwwaaalnwEOSwy\n",
      "Epoch:  48 | Train loss: 1.619 | Val loss: 1.948 | Gen: ayy inaay ooooycyEOSeetaty isay ellwayEOSwaway\n",
      "Epoch:  49 | Train loss: 1.623 | Val loss: 1.893 | Gen: ayy inay ongeray-oay isay ilwlayEOSidwayy\n",
      "Epoch:  50 | Train loss: 1.618 | Val loss: 1.959 | Gen: ayy inwaay ectntatitny illsyy iwlawyiy\n",
      "Epoch:  51 | Train loss: 1.633 | Val loss: 1.949 | Gen: ay iray ongedencay isay iilaayiidhhiydday\n",
      "Epoch:  52 | Train loss: 1.619 | Val loss: 1.914 | Gen: ay iray onterayy isay iliiiwilaaay\n",
      "Validation loss has not improved in 10 epochs, stopping early\n",
      "Obtained lowest validation loss of: 1.8795731365680695\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tay iray ontintitiontititity islay origiggay\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "\n",
    "trans32_args_l = AttrDict()\n",
    "args_dict = {\n",
    "              'data_file_name': 'pig_latin_large', # Increased data set size\n",
    "              'cuda':False, \n",
    "              'nepochs':100,\n",
    "              'checkpoint_dir':\"checkpoints\", \n",
    "              'learning_rate':5e-4,\n",
    "              'early_stopping_patience': 10,\n",
    "              'lr_decay':0.99,\n",
    "              'batch_size': 512,\n",
    "              'hidden_size': 32,\n",
    "              'encoder_type': 'transformer',\n",
    "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
    "              'num_transformer_layers': 3,\n",
    "}\n",
    "trans32_args_l.update(args_dict)\n",
    "print_opts(trans32_args_l)\n",
    "\n",
    "trans32_encoder_l, trans32_decoder_l, trans32_losses_l = train(trans32_args_l)\n",
    "\n",
    "translated = translate_sentence(TEST_SENTENCE, trans32_encoder_l, trans32_decoder_l, None, trans32_args_l)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "SmoTgrDcr_dw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_small                        \n",
      "                                   cuda: 0                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.0005                                 \n",
      "                early_stopping_patience: 20                                     \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 64                                     \n",
      "                            hidden_size: 64                                     \n",
      "                           encoder_type: transformer                            \n",
      "                           decoder_type: transformer                            \n",
      "                 num_transformer_layers: 3                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('fears', 'earsfay')\n",
      "('utility', 'utilityway')\n",
      "('accusation', 'accusationway')\n",
      "('i', 'iway')\n",
      "('delirium', 'eliriumday')\n",
      "Num unique word pairs: 3198\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Epoch:   0 | Train loss: 3.470 | Val loss: 2.899 | Gen: ay t-t-t-t-tg-t-ty irrrrrrrrrrrrrongr-o llllllllllllllllllll enaiyaoyy\n",
      "Epoch:   1 | Train loss: 2.688 | Val loss: 2.571 | Gen: ay ay antt-aoyaaaaaayaaaaa ay ay\n",
      "Epoch:   2 | Train loss: 2.362 | Val loss: 2.359 | Gen: ehahyhyy ay ooglngglll--lllllll- iy ininay\n",
      "Epoch:   3 | Train loss: 2.233 | Val loss: 2.206 | Gen: ehhhyyyyehhhyy ilway ononondononondongdon iy ionyniy\n",
      "Epoch:   4 | Train loss: 2.103 | Val loss: 2.176 | Gen: ehhhaaaaaay isway ondonddoooocooooddoo ay inghayy\n",
      "Epoch:   5 | Train loss: 2.033 | Val loss: 2.163 | Gen: eteeeeeeeeeeey ayEOSty ondiiccciidiyinyyyy iy inanyiy\n",
      "Epoch:   6 | Train loss: 2.006 | Val loss: 2.098 | Gen: ehhhhhhhhhhhay ay onionccayn-gaa-nnccc ayny ionay\n",
      "Epoch:   7 | Train loss: 1.950 | Val loss: 2.093 | Gen: ehay ay oiouayiw-ooooy iyay ongy\n",
      "Epoch:   8 | Train loss: 1.910 | Val loss: 2.039 | Gen: ehaEOShhEOSEOSEOSEOSEOSEOSEOShaaay ay oncnnnwdoyywynnnyyyy ilway inghay\n",
      "Epoch:   9 | Train loss: 1.868 | Val loss: 2.015 | Gen: ehhhhhhhhhhhhhhay ayay onddnwdwnooyddanoynd iway ingway\n",
      "Epoch:  10 | Train loss: 1.838 | Val loss: 1.967 | Gen: ehay iswaway incay-iouioncay-itay iiiiiway inghay\n",
      "Epoch:  11 | Train loss: 1.835 | Val loss: 2.088 | Gen: ay ay odayyyyyyy inay ongaay\n",
      "Epoch:  12 | Train loss: 1.875 | Val loss: 1.989 | Gen: ehay ay ottttty ifay onay\n",
      "Epoch:  13 | Train loss: 1.811 | Val loss: 1.968 | Gen: ehthhhhhyhhyyEOShhhhhh mmay ondsay isay ongnayay\n",
      "Epoch:  14 | Train loss: 1.797 | Val loss: 1.944 | Gen: ehhahahaahahaaay amay ontaaaaaaay isay ongway-ongway\n",
      "Epoch:  15 | Train loss: 1.776 | Val loss: 1.920 | Gen: esahay away onstway-onday ay ongaay-ongway\n",
      "Epoch:  16 | Train loss: 1.707 | Val loss: 1.944 | Gen: ehahy away ontway-oooonoonay EOSfffffffay ononyy-o-oy\n",
      "Epoch:  17 | Train loss: 1.751 | Val loss: 1.972 | Gen: EOSshaay ay ontray iiaay ongay-oooooy\n",
      "Epoch:  18 | Train loss: 1.727 | Val loss: 1.919 | Gen: eyy ay onsayoyy fy ongwayooowwEOSy\n",
      "Epoch:  19 | Train loss: 1.731 | Val loss: 2.031 | Gen: ay away oyoooyoyyyy ay ongaygoy\n",
      "Epoch:  20 | Train loss: 1.743 | Val loss: 1.939 | Gen: ay ay otay-ooooooooooooooo ay-iay-iay ongay-oooonoooongay\n",
      "Epoch:  21 | Train loss: 1.692 | Val loss: 1.879 | Gen: ay away onaaayooyyyyyyyEOSy ay ongsay\n",
      "Epoch:  22 | Train loss: 1.639 | Val loss: 1.861 | Gen: ewwy ay ongtayyooyy ay ononay\n",
      "Epoch:  23 | Train loss: 1.637 | Val loss: 1.846 | Gen: ewEOSy ay onoosssaag-nEOSooaEOSooo isaay ongsay-ooooooooooooo\n",
      "Epoch:  24 | Train loss: 1.632 | Val loss: 1.834 | Gen: ehay away ontayooyooyyyyyy isayyasyyy onooorroaooorranwwww\n",
      "Epoch:  25 | Train loss: 1.609 | Val loss: 1.841 | Gen: hhay ay ondayy ar-yray ongsay-ooooooooooooo\n",
      "Epoch:  26 | Train loss: 1.572 | Val loss: 1.782 | Gen: hhy away ontay-otay oyEOSy onsaay\n",
      "Epoch:  27 | Train loss: 1.540 | Val loss: 1.772 | Gen: hhay away ontay-ony ey-aEOSayy onaay\n",
      "Epoch:  28 | Train loss: 1.509 | Val loss: 1.743 | Gen: ehayyy away ontay-otay isaay-y ongsay-onay\n",
      "Epoch:  29 | Train loss: 1.502 | Val loss: 1.796 | Gen: hhay ay ontay-ontay ay onswaayynyy\n",
      "Epoch:  30 | Train loss: 1.498 | Val loss: 1.755 | Gen: ehaay aray ontay-otay isaay-isay onssay-ooooooooooooo\n",
      "Epoch:  31 | Train loss: 1.494 | Val loss: 1.792 | Gen: hhayy arway oncococcacnyyEOSEOSaoayo isay okayayy\n",
      "Epoch:  32 | Train loss: 1.554 | Val loss: 1.876 | Gen: hhay aray onssay-onday ay oraay\n",
      "Epoch:  33 | Train loss: 1.567 | Val loss: 1.833 | Gen: ehay ary oaaooyty ay-ayayy owaway\n",
      "Epoch:  34 | Train loss: 1.528 | Val loss: 1.818 | Gen: eyay arwy oncday--oddtyyyay isaa-isay owewway\n",
      "Epoch:  35 | Train loss: 1.505 | Val loss: 1.758 | Gen: ehaay arway oncoadaaay isay oway\n",
      "Epoch:  36 | Train loss: 1.495 | Val loss: 1.787 | Gen: eway-EOSEOSyy arway-ay ongay-onday issaywy oway-oway\n",
      "Epoch:  37 | Train loss: 1.456 | Val loss: 1.764 | Gen: awyy arway onganaay issaswy oway-oway\n",
      "Epoch:  38 | Train loss: 1.467 | Val loss: 1.837 | Gen: eway arway oncanc-aaay ayy ongway\n",
      "Epoch:  39 | Train loss: 1.509 | Val loss: 1.834 | Gen: eay-aa-hhy aray onaay ay ongkayyonggEOSEOSsgy\n",
      "Epoch:  40 | Train loss: 1.628 | Val loss: 1.810 | Gen: thaayyy aray oncay-otayay isay-iy ongnay\n",
      "Epoch:  41 | Train loss: 1.612 | Val loss: 1.886 | Gen: ehaay ay ongskngsay ay-ay okkooooaaaaawwway\n",
      "Epoch:  42 | Train loss: 1.594 | Val loss: 1.804 | Gen: ehhahahay ay oncontconty ay onsnsay\n",
      "Epoch:  43 | Train loss: 1.539 | Val loss: 1.807 | Gen: ehahahay ay oncaycayyyyEOSay ay ongkkrryonyy\n",
      "Epoch:  44 | Train loss: 1.499 | Val loss: 1.782 | Gen: ehahy ay oncknconcanconcay ay onway\n",
      "Epoch:  45 | Train loss: 1.467 | Val loss: 1.737 | Gen: ehahay ay oncaycaycyyEOSay ay onway\n",
      "Epoch:  46 | Train loss: 1.440 | Val loss: 1.733 | Gen: ehahay ay oncancanaanaancay ay onay\n",
      "Epoch:  47 | Train loss: 1.422 | Val loss: 1.718 | Gen: ehahay ay oncancanaanaancay ay onay\n",
      "Epoch:  48 | Train loss: 1.411 | Val loss: 1.708 | Gen: ehahay ay oncknconcancancay ay onway\n",
      "Epoch:  49 | Train loss: 1.399 | Val loss: 1.694 | Gen: ehhhhhyy ay oncancanaay ay onnkaaoonaay\n",
      "Obtained lowest validation loss of: 1.6943262263042171\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tehhhhhyy ay oncancanaay ay onnkaaoonaay\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "\n",
    "trans64_args_s = AttrDict()\n",
    "args_dict = {\n",
    "              'data_file_name': 'pig_latin_small',\n",
    "              'cuda':False, \n",
    "              'nepochs':50, \n",
    "              'checkpoint_dir':\"checkpoints\", \n",
    "              'learning_rate':5e-4,\n",
    "              'early_stopping_patience': 20,\n",
    "              'lr_decay':0.99,\n",
    "              'batch_size': 64, \n",
    "              'hidden_size': 64, # Increased model size\n",
    "              'encoder_type': 'transformer',\n",
    "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
    "              'num_transformer_layers': 3,\n",
    "}\n",
    "trans64_args_s.update(args_dict)\n",
    "print_opts(trans64_args_s)\n",
    "\n",
    "trans64_encoder_s, trans64_decoder_s, trans64_losses_s = train(trans64_args_s)\n",
    "\n",
    "translated = translate_sentence(TEST_SENTENCE, trans64_encoder_s, trans64_decoder_s, None, trans64_args_s)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "dardK4RWvUWV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "                                      Opts                                      \n",
      "--------------------------------------------------------------------------------\n",
      "                         data_file_name: pig_latin_large                        \n",
      "                                   cuda: 0                                      \n",
      "                                nepochs: 50                                     \n",
      "                         checkpoint_dir: checkpoints                            \n",
      "                          learning_rate: 0.0005                                 \n",
      "                early_stopping_patience: 20                                     \n",
      "                               lr_decay: 0.99                                   \n",
      "                             batch_size: 512                                    \n",
      "                            hidden_size: 64                                     \n",
      "                           encoder_type: transformer                            \n",
      "                           decoder_type: transformer                            \n",
      "                 num_transformer_layers: 3                                      \n",
      "================================================================================\n",
      "================================================================================\n",
      "                                   Data Stats                                   \n",
      "--------------------------------------------------------------------------------\n",
      "('tutor', 'utortay')\n",
      "('es', 'esway')\n",
      "('dams', 'amsday')\n",
      "('gala', 'alagay')\n",
      "('rehearsal', 'ehearsalray')\n",
      "Num unique word pairs: 22402\n",
      "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
      "Vocab size: 29\n",
      "================================================================================\n",
      "Epoch:   0 | Train loss: 3.220 | Val loss: 2.779 | Gen: oay aaaaayaaaaaaaaaaaaaa -la-oyyoopy-a-eeeoy- tay aay\n",
      "Epoch:   1 | Train loss: 2.516 | Val loss: 2.406 | Gen: -y aayayyyaaay oeepcctcppotococccoo aty owayEOSooooway\n",
      "Epoch:   2 | Train loss: 2.223 | Val loss: 2.322 | Gen: otay unway onpooooooooooootoooo usssssssssssssssssss unay-oyyuuyyuyay\n",
      "Epoch:   3 | Train loss: 2.078 | Val loss: 2.215 | Gen: otgay unr-ar-ay-ayyyyaay-a ototototooooootttttt ay oway-ooooooooooooooo\n",
      "Epoch:   4 | Train loss: 1.998 | Val loss: 2.086 | Gen: uath arayEOSayEOSay otat-oootttotttttttt asay orgrorayyyyooooyy\n",
      "Epoch:   5 | Train loss: 1.902 | Val loss: 2.065 | Gen: eshaay aay otoooooonstooaatooam asay grggggggraggyggggggg\n",
      "Epoch:   6 | Train loss: 1.853 | Val loss: 1.996 | Gen: ahay aray-ay-ay ontay-oncccontay issay olrry\n",
      "Epoch:   7 | Train loss: 1.798 | Val loss: 1.966 | Gen: hhthhthahthah aay-ay-ay otggtnnnnnnptnnnyoy- issay onrregaaayyy\n",
      "Epoch:   8 | Train loss: 1.757 | Val loss: 1.963 | Gen: uthhay aay-ay otooaaoanaonotocccyn issay angaay--yEOSEOSEOSyy\n",
      "Epoch:   9 | Train loss: 1.780 | Val loss: 1.961 | Gen: uththayy aay intintintiniiiynyytt awEOSy eray-era-ay\n",
      "Epoch:  10 | Train loss: 1.747 | Val loss: 1.921 | Gen: atttyhyaaty aray-ay-ay otgooooooooooooogy ay aray-ay-y\n",
      "Epoch:  11 | Train loss: 1.703 | Val loss: 1.920 | Gen: atttty aray ottttttttttttttttttt osay orgray-ongay\n",
      "Epoch:  12 | Train loss: 1.682 | Val loss: 1.881 | Gen: athay-ay aray-ay-ay ontootontononnootooo osay onghay-oray\n",
      "Epoch:  13 | Train loss: 1.706 | Val loss: 2.050 | Gen: atay ary intway-intinway ssay aray\n",
      "Epoch:  14 | Train loss: 1.668 | Val loss: 1.913 | Gen: athay-ay away-ay-ay ongay-ongway osssssay oway-oongayEOSooongron\n",
      "Epoch:  15 | Train loss: 1.607 | Val loss: 1.908 | Gen: ay arrrrarrrrrrrryrrry ongay-oooooooooooooo ay oway-oway\n",
      "Epoch:  16 | Train loss: 1.579 | Val loss: 1.855 | Gen: atay airrrrrrrrrrrrrrrrrr oogoooooooooooonay issssssssssssssssay oway-orryy\n",
      "Epoch:  17 | Train loss: 1.578 | Val loss: 1.864 | Gen: utethahauththaytyhtt away-ay ontiotay-ooooooooooo ossay eray-ingay\n",
      "Epoch:  18 | Train loss: 1.535 | Val loss: 1.833 | Gen: athay arrrrrry-ay ontiogaooncoctaay ay-yy onarayry\n",
      "Epoch:  19 | Train loss: 1.501 | Val loss: 1.840 | Gen: athay away-ay ontionway ussway owar-yroy\n",
      "Epoch:  20 | Train loss: 1.471 | Val loss: 1.819 | Gen: athay away-ay onday-onday ussay away\n",
      "Epoch:  21 | Train loss: 1.488 | Val loss: 1.874 | Gen: ay away-ay onday-ondndndndndndn ayayy ingway-inway\n",
      "Epoch:  22 | Train loss: 1.475 | Val loss: 1.863 | Gen: ay away-ay ontay-ondnton-dntta- ay ingray-inway\n",
      "Epoch:  23 | Train loss: 1.427 | Val loss: 1.839 | Gen: ay away-ay onday-ondayEOSonay uwayy ongay-ongway\n",
      "Epoch:  24 | Train loss: 1.408 | Val loss: 1.847 | Gen: ayyhhyy away-ay onday-ondayEOSnay awwy ingway-inwny\n",
      "Epoch:  25 | Train loss: 1.387 | Val loss: 1.827 | Gen: ayyy away-ay onday-onnnnnnnnaa-EOSn uway ingway-inway\n",
      "Epoch:  26 | Train loss: 1.363 | Val loss: 1.874 | Gen: ay away-ay ontay-ontanayy iwaawwyy ingrinray\n",
      "Epoch:  27 | Train loss: 1.440 | Val loss: 1.934 | Gen: ayy away-ay onnaaaoaaaaanny iswayy ingwaaaay\n",
      "Epoch:  28 | Train loss: 1.489 | Val loss: 1.891 | Gen: aytttttty away-ay onday-ontay-onttnnna iaaaaay uwdeaayy\n",
      "Epoch:  29 | Train loss: 1.408 | Val loss: 1.871 | Gen: ay away-ay untacasctywwwttwyyyy awayyy eway-esayEOSey\n",
      "Epoch:  30 | Train loss: 1.375 | Val loss: 1.804 | Gen: ay away-ary onday-ondnynonday aygyy ilay-inwyEOSy\n",
      "Epoch:  31 | Train loss: 1.343 | Val loss: 1.826 | Gen: ay away-ay ondic-onaayoEOSyEOSEOSy iaayy oway-ongway\n",
      "Epoch:  32 | Train loss: 1.322 | Val loss: 1.784 | Gen: ay away-ay ondtccynccoccccyyyyy aywy oway-ongway\n",
      "Epoch:  33 | Train loss: 1.298 | Val loss: 1.780 | Gen: ay away onstcccaaay iaay oway-ongway\n",
      "Epoch:  34 | Train loss: 1.275 | Val loss: 1.827 | Gen: ay away-ay onttictay iawy isgray-iy\n",
      "Epoch:  35 | Train loss: 1.273 | Val loss: 1.815 | Gen: aay away-ay onticay-ontontoy isway isgray-inway\n",
      "Epoch:  36 | Train loss: 1.250 | Val loss: 1.843 | Gen: ay away-ay onttctaytntootty iay ingray-inway\n",
      "Epoch:  37 | Train loss: 1.265 | Val loss: 1.905 | Gen: ay aray-way ondtintay ay indinwray\n",
      "Epoch:  38 | Train loss: 1.267 | Val loss: 1.868 | Gen: ay aray-ay onday-onc-naay aswy okkay-odd--y\n",
      "Epoch:  39 | Train loss: 1.240 | Val loss: 1.779 | Gen: ay away-ay onstincccsss-ny aywy okkanay-EOSnynEOSnwyaayy\n",
      "Epoch:  40 | Train loss: 1.223 | Val loss: 1.835 | Gen: ayy arway ondiynnnyEOSnyyyyy aswy okkanaay\n",
      "Epoch:  41 | Train loss: 1.222 | Val loss: 1.838 | Gen: ayEOSwy aray-ay onstinaay isaay okkiygy-EOSoynyyyyyyyw\n",
      "Epoch:  42 | Train loss: 1.211 | Val loss: 1.789 | Gen: ayEOSwy aray-ay onstitattaaaaaaay isay erknwwaaaywy\n",
      "Epoch:  43 | Train loss: 1.175 | Val loss: 1.801 | Gen: ayy away-ay onsttaaaay iaaay ewkinayyeywEOSyewyy\n",
      "Epoch:  44 | Train loss: 1.155 | Val loss: 1.771 | Gen: ay away-ay onstttattay saay erknway-eiiy\n",
      "Epoch:  45 | Train loss: 1.153 | Val loss: 1.839 | Gen: ay aray onday-oncay saay erkiaaaaayy\n",
      "Epoch:  46 | Train loss: 1.153 | Val loss: 1.822 | Gen: ay aray-ay ondisa-ssssssssssssn isayy okkingay\n",
      "Epoch:  47 | Train loss: 1.143 | Val loss: 1.889 | Gen: ay aray-ay ondttnttay sayy oendwwwaaawwaaaaaaay\n",
      "Epoch:  48 | Train loss: 1.156 | Val loss: 1.797 | Gen: ay awway ontttictay saay isdinnwaawwwwawwwwy\n",
      "Epoch:  49 | Train loss: 1.141 | Val loss: 1.830 | Gen: ay away-ay ondccitaaay saay ukknaaayy\n",
      "Obtained lowest validation loss of: 1.7711772580559437\n",
      "source:\t\tthe air conditioning is working \n",
      "translated:\tay away-ay ondccitaaay saay ukknaaayy\n"
     ]
    }
   ],
   "source": [
    "TEST_SENTENCE = 'the air conditioning is working'\n",
    "\n",
    "trans64_args_l = AttrDict()\n",
    "args_dict = {\n",
    "              'data_file_name': 'pig_latin_large', # Increased data set size\n",
    "              'cuda':False, \n",
    "              'nepochs':50,\n",
    "              'checkpoint_dir':\"checkpoints\", \n",
    "              'learning_rate':5e-4,\n",
    "              'early_stopping_patience': 20,\n",
    "              'lr_decay':0.99,\n",
    "              'batch_size': 512, \n",
    "              'hidden_size': 64, # Increased model size\n",
    "              'encoder_type': 'transformer',\n",
    "              'decoder_type': 'transformer', # options: rnn / rnn_attention / transformer\n",
    "              'num_transformer_layers': 3,\n",
    "}\n",
    "trans64_args_l.update(args_dict)\n",
    "print_opts(trans64_args_l)\n",
    "\n",
    "trans64_encoder_l, trans64_decoder_l, trans64_losses_l = train(trans64_args_l)\n",
    "\n",
    "translated = translate_sentence(TEST_SENTENCE, trans64_encoder_l, trans64_decoder_l, None, trans64_args_l)\n",
    "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(TEST_SENTENCE, translated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSSyiG39vVlN"
   },
   "source": [
    "The following cell generates two loss plots. In the first plot, we compare the effects of increasing dataset size. In the second plot, we compare the effects of increasing model size. Include both plots in your report, and include your analysis of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "-Ql0pxrEvVP6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_loss_comparison_by_dataset(trans32_losses_s, trans32_losses_l, trans64_losses_s, trans64_losses_l, trans32_args_s, trans32_args_l, trans64_args_s, trans64_args_l, 'trans_by_dataset')\n",
    "save_loss_comparison_by_hidden(trans32_losses_s, trans32_losses_l, trans64_losses_s, trans64_losses_l, trans32_args_s, trans32_args_l, trans64_args_s, trans64_args_l, 'trans_by_hidden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBnBXRG8mvcn"
   },
   "source": [
    "# Optional: Attention Visualizations\n",
    "\n",
    "One of the benefits of using attention is that it allows us to gain insight into the inner workings of the model.\n",
    "\n",
    "By visualizing the attention weights generated for the input tokens in each decoder step, we can see where the model focuses while producing each output token.\n",
    "\n",
    "The code in this section loads the model you trained from the previous section and uses it to translate a given set of words: it prints the translations and display heatmaps to show how attention is used at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqEC0vN9mvpV"
   },
   "source": [
    "## Step 1: Visualize Attention Masks\n",
    "Play around with visualizing attention maps generated by the previous two models you've trained. Inspect visualizations in one success and one failure case for both models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Dkfz-u-MtudL"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-5075212dbc3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mTEST_WORD_ATTN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'street'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvisualize_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_WORD_ATTN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_attn_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_attn_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "TEST_WORD_ATTN = 'street'\n",
    "visualize_attention(TEST_WORD_ATTN, rnn_attn_encoder, rnn_attn_decoder, None, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ssa7g35zt2yj"
   },
   "outputs": [],
   "source": [
    "TEST_WORD_ATTN = 'street'\n",
    "visualize_attention(TEST_WORD_ATTN, transformer_encoder, transformer_decoder, None, args, )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4BIpGwANoQOg",
    "pbvpn4MaV0I1",
    "bRWfRdmVVjUl",
    "0yh08KhgnA30",
    "ecEq4TP2lZ4Z",
    "9tcpUFKqo2Oi",
    "z1hDi020rT36",
    "MBnBXRG8mvcn"
   ],
   "name": "nmt.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
